{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qgXboN5aN9MX"
   },
   "source": [
    "# Proximal Policy Optimization with Brax\n",
    "\n",
    "Proximal Policy Optimization (PPO) is an on-policy, model-free Reinforcement Learning algorithm.\n",
    "\n",
    "As a simulator of choice we use [Brax](https://github.com/google/brax), since it is written in JAX and supports several continuous-control environments. Also, it is blazingly fast!\n",
    "\n",
    "For particular details about PPO implementation, you can read the original [PPO paper from arxiv](https://arxiv.org/abs/1707.06347) or [this awesome article](https://iclr-blog-track.github.io/2022/03/25/ppo-implementation-details)\n",
    "\n",
    "\n",
    "!!! cite \"Reference\"\n",
    "```bibtex\n",
    "@misc{schulman2017proximal,\n",
    "      title={Proximal Policy Optimization Algorithms},\n",
    "      author={John Schulman and Filip Wolski and Prafulla Dhariwal and Alec Radford and Oleg Klimov},\n",
    "      year={2017},\n",
    "      eprint={1707.06347},\n",
    "      archivePrefix={arXiv},\n",
    "      primaryClass={cs.LG}\n",
    "}\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AUKGhZ1WB_K-"
   },
   "source": [
    "Include all the libraries we need. In fact, we have only three dependencies: JAX, Brax and Equinox."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "sFx5yceozxFi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "3893f674-1c8d-4ce7-8d73-ab3525e5a838",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:10.334786798Z",
     "start_time": "2023-12-07T15:17:09.062735088Z"
    }
   },
   "outputs": [],
   "source": [
    "# JAX, Autograd and XLA\n",
    "# GitHub: https://github.com/google/jax\n",
    "import jax\n",
    "from jax import lax\n",
    "from jax import random as jr\n",
    "from jax import numpy as jnp\n",
    "from jax import tree_util as jtu\n",
    "\n",
    "# Optax, optimizers for machine learning in JAX\n",
    "# GitHub: https://github.com/google-deepmind/optax\n",
    "import optax\n",
    "\n",
    "# Equinox, awesome library for (awesome) neural networks in JAX\n",
    "# GitHub: https://github.com/patrick-kidger/equinox\n",
    "# Arxiv: https://arxiv.org/abs/2111.00254\n",
    "import equinox as eqx\n",
    "\n",
    "# Brax, vectorized continuous RL environments, in JAX\n",
    "# GitHub: https://github.com/google/brax\n",
    "# Arxiv: https://arxiv.org/abs/2106.13281\n",
    "from brax import envs\n",
    "\n",
    "from collections.abc import Callable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jTSLKsU-yyja"
   },
   "source": [
    "A few utility functions that are similar to other Equinox filtered wrappers.\n",
    "\n",
    "In the implementation we are constantly using lax.scan, and hence wrapping it, so that it can consume partially-static PyTrees seems nice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "Ca2ekgyQD99U",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:10.341023730Z",
     "start_time": "2023-12-07T15:17:10.338410126Z"
    }
   },
   "outputs": [],
   "source": [
    "# A few of utility functions, that automatically partition PyTrees\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def filter_scan(f: Callable, init, xs, *args, **kwargs):\n",
    "    \"\"\"Same as lax.scan, but allows to have eqx.Module in carry\"\"\"\n",
    "    init_dynamic_carry, static_carry = eqx.partition(init, eqx.is_array)\n",
    "\n",
    "    def to_scan(dynamic_carry, x):\n",
    "        carry = eqx.combine(dynamic_carry, static_carry)\n",
    "        new_carry, out = f(carry, x)\n",
    "        dynamic_new_carry, _ = eqx.partition(new_carry, eqx.is_array)\n",
    "        return dynamic_new_carry, out\n",
    "\n",
    "    out_carry, out_ys = lax.scan(to_scan, init_dynamic_carry, xs, *args, **kwargs)\n",
    "    return eqx.combine(out_carry, static_carry), out_ys\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def filter_cond(pred, true_f: Callable, false_f: Callable, *args):\n",
    "    \"\"\"Same as lax.cond, but allows to return eqx.Module\"\"\"\n",
    "    dynamic_true, static_true = eqx.partition(true_f(*args), eqx.is_array)\n",
    "    dynamic_false, static_false = eqx.partition(false_f(*args), eqx.is_array)\n",
    "\n",
    "    static_part = eqx.error_if(\n",
    "        static_true,\n",
    "        static_true != static_false,\n",
    "        \"Filtered conditional arguments should have the same static part\",\n",
    "    )\n",
    "\n",
    "    dynamic_part = lax.cond(pred, lambda *_: dynamic_true, lambda *_: dynamic_false)\n",
    "    return eqx.combine(dynamic_part, static_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "25TkGpz3zioH"
   },
   "source": [
    "A few useful dataclasses ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "KyUpyYNVuYQZ",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:10.358759270Z",
     "start_time": "2023-12-07T15:17:10.343023649Z"
    }
   },
   "outputs": [],
   "source": [
    "class LogNormalDistribution(eqx.Module):\n",
    "    \"\"\"Multivariate Log Normal distribution with diagonal covariance\"\"\"\n",
    "\n",
    "    mean: jax.Array\n",
    "    log_std: jax.Array\n",
    "\n",
    "    def get_pdf(self, value):\n",
    "        value = eqx.error_if(\n",
    "            value,\n",
    "            value.shape != self.mean.shape,\n",
    "            \"Wrong shapes for the mean/value of action distr\",\n",
    "        )\n",
    "        value = eqx.error_if(\n",
    "            value,\n",
    "            value.shape != self.log_std.shape,\n",
    "            \"Wrong shapes for the std/value of action distr\",\n",
    "        )\n",
    "\n",
    "        normalized = (value - self.mean) / jnp.exp(self.log_std)\n",
    "        return jax.scipy.stats.norm.logpdf(normalized).sum()\n",
    "\n",
    "    def sample(self, key: jr.PRNGKey):\n",
    "        return jr.normal(key, self.mean.shape) * jnp.exp(self.log_std) + self.mean\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.log_std.sum() * 0.5  # entropy without constant factor\n",
    "\n",
    "\n",
    "class Action(eqx.Module):\n",
    "    \"\"\"\n",
    "    Action class represents a single action taken by an agent.\n",
    "    Additionally stores some useful data.\n",
    "\n",
    "    raw: action that was a direct output of an Actor-Critic model\n",
    "    transformed: action that was applied on the environment\n",
    "    distr: distribution from which raw action was sampled\n",
    "    \"\"\"\n",
    "\n",
    "    raw: jax.Array = None\n",
    "    transformed: jax.Array = None\n",
    "    distr: LogNormalDistribution = None\n",
    "\n",
    "    def postprocess(self, apply: Callable):\n",
    "        return Action(\n",
    "            raw=self.raw, transformed=apply(self.transformed), distr=self.distr\n",
    "        )\n",
    "\n",
    "\n",
    "class ValueRange(eqx.Module):\n",
    "    low: jax.Array\n",
    "    high: jax.Array"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7IHRqFzZZp01"
   },
   "source": [
    "Let's make all the neural network classes. The approach is standard for Actor-Critic methods:\n",
    "* **Critic**, that estimates the TD residual (sum of discounted rewards for all the future), given the state (observation).\n",
    "* **Actor**, that outputs policy action given the state. In our implementation, **Actor** consists of the **MeanNetwork** that outputs mean of the action distribution given the current state, and the **log_std** parameter, that is trained, but does not depend on the state. This is an implementation detail: frequently, **log_std** is also predicted by a neural network, and depends on the state, but we decided to keep it a simple trainable parameter. The output of the **Actor** is an **Action** that contains the output distribution and the sampled action.\n",
    "* **ActorCritic** is just an abstraction, that allows to address both **Actor** and **Critic** via a single interface.\n",
    "\n",
    "\n",
    "Sizes of layers for all the MLPs (neural networks) are chosen arbitrarily; Feel free to change them and look at the effect on the performance :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "7EvryteDt91d",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:10.403890815Z",
     "start_time": "2023-12-07T15:17:10.358210526Z"
    }
   },
   "outputs": [],
   "source": [
    "class Critic(eqx.Module):\n",
    "    \"\"\"MLP, that outputs the TD residual for the given state.\"\"\"\n",
    "\n",
    "    structure: list\n",
    "\n",
    "    def __init__(self, key: jr.PRNGKey, observation_size: int):\n",
    "        output_size = 1  # output is the value, TD residual, always a single output\n",
    "\n",
    "        key1, key2, key3, key4 = jr.split(key, 4)\n",
    "        self.structure = [\n",
    "            eqx.nn.Linear(observation_size, 64, key=key1),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, 64, key=key2),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, 64, key=key3),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, output_size, key=key4),\n",
    "        ]\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for operator in self.structure:\n",
    "            x = operator(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MeanNetwork(eqx.Module):\n",
    "    \"\"\"MLP, that outputs a mean for the Action distribution, given state.\"\"\"\n",
    "\n",
    "    structure: list\n",
    "\n",
    "    def __init__(self, key: jr.PRNGKey, observation_size: int, action_size: int):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        self.structure = [\n",
    "            eqx.nn.Linear(observation_size, 64, key=key1),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, 64, key=key2),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, 64, key=key3),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, action_size, key=key4),\n",
    "        ]\n",
    "\n",
    "        # scaling down the weights of the output layer improves performance\n",
    "        self.structure = eqx.tree_at(\n",
    "            where=lambda s: s[-1].weight,\n",
    "            pytree=self.structure,\n",
    "            replace_fn=lambda weight: weight * 0.01,\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for operator in self.structure:\n",
    "            x = operator(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Actor(eqx.Module):\n",
    "    \"\"\"A module, that outputs action distribution for a particular state.\"\"\"\n",
    "\n",
    "    mean_network: MeanNetwork\n",
    "    log_std: jax.Array  # Trainable array\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jr.PRNGKey,\n",
    "        observation_size: int,\n",
    "        action_size: int,\n",
    "        initial_std: float,\n",
    "    ):\n",
    "        self.mean_network = MeanNetwork(key, observation_size, action_size)\n",
    "        self.log_std = jnp.ones((action_size,)) * jnp.log(initial_std)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        return LogNormalDistribution(self.mean_network(x), self.log_std)\n",
    "\n",
    "\n",
    "class ActorCritic(eqx.Module):\n",
    "    \"\"\"Allows to use functionality of both Actor and Critic via the same entity.\"\"\"\n",
    "\n",
    "    obs_size: int = eqx.field(static=True)\n",
    "    act_size: int = eqx.field(static=True)\n",
    "    critic: Critic\n",
    "    actor: Actor\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        key: jr.PRNGKey,\n",
    "        observation_size: int,\n",
    "        action_size: int,\n",
    "        initial_actor_std: float = 0.5,\n",
    "    ):\n",
    "        self.obs_size = observation_size\n",
    "        self.act_size = action_size\n",
    "\n",
    "        key1, key2 = jax.random.split(key, 2)\n",
    "        self.critic = Critic(key1, observation_size)\n",
    "        self.actor = Actor(key2, observation_size, action_size, initial_actor_std)\n",
    "\n",
    "    def get_value(self, observation):\n",
    "        return self.critic(observation)\n",
    "\n",
    "    def get_action(self, key: jr.PRNGKey, observation):\n",
    "        distr = self.actor(observation)\n",
    "        action = distr.sample(key)\n",
    "        return Action(raw=action, transformed=action, distr=distr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLz0jK0Gc5kd"
   },
   "source": [
    "The next thing is wrappers for the Agent.\n",
    "\n",
    "One could think that all you need to train the model, one already has: indeed, you can train the **ActorCritic** by simply passing the observations from the environment, and then use the outputs as the actions.\n",
    "\n",
    "But, frequently, there are some problem-specific constraints on both observations/actions of the environment. Thus, it makes sense to preprocess the observations, before passing them to the **ActorCritic**, and postprocess the outputs, so that it is easier for underlying MLP to learn.\n",
    "\n",
    "* **BaseWrapper** is a parent class for any Wrapper. Any wrapper is some simple algorithm, that does pre- or postprocessing of the **ActorCritic** inputs and outputs.\n",
    "\n",
    "* **Agent** is a transparent wrapper, that allows to use **ActorCritic** as is, without any additional processing. It is needed so that there is the same interface for addressing the model with and without wrappers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "-eiaCC5qv6Lo",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:10.406495834Z",
     "start_time": "2023-12-07T15:17:10.363887415Z"
    }
   },
   "outputs": [],
   "source": [
    "class BaseWrapper(eqx.Module):\n",
    "    \"\"\"\n",
    "    Parent class of any wrapper.\n",
    "\n",
    "    next: the wrapper (or anything else) that is considered the next one.\n",
    "    params: persistent properties (parameters) of this particular wrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    next: eqx.Module = None\n",
    "    params: eqx.Module = None\n",
    "\n",
    "    def get_trainable(self):\n",
    "        # returns a PyTree with the same structure as self,\n",
    "        # but every leaf except children of Actor-Critic are replaced by None\n",
    "        return eqx.filter(\n",
    "            self,\n",
    "            filter_spec=lambda x: isinstance(x, ActorCritic),\n",
    "            is_leaf=lambda x: isinstance(x, ActorCritic),\n",
    "        )\n",
    "\n",
    "    def get_obs_size(self):\n",
    "        return self.next.get_obs_size()\n",
    "\n",
    "    def get_act_size(self):\n",
    "        return self.next.get_act_size()\n",
    "\n",
    "    def set_next(self, new_next):\n",
    "        return eqx.tree_at(lambda wrapper: wrapper.next, self, new_next)\n",
    "\n",
    "    def get_action(self, key: jr.PRNGKey, observation):\n",
    "        out, new_next = self.next.get_action(key, observation)\n",
    "        return out, self.set_next(new_next)\n",
    "\n",
    "    def get_value(self, observation):\n",
    "        out, new_next = self.next.get_value(observation)\n",
    "        return out, self.set_next(new_next)\n",
    "\n",
    "    def config(self, **kwargs):\n",
    "        return self.set_next(self.next.config(**kwargs))\n",
    "\n",
    "\n",
    "class Agent(BaseWrapper):\n",
    "    \"\"\"\n",
    "    Transparent, unmutable wrapper.\n",
    "    Used so that even pure Actor-Critic can be used as a wrapper.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        self.next = ActorCritic(*args, **kwargs)\n",
    "\n",
    "    def set_next(self, _):\n",
    "        # unmutable -> don't change the self.next\n",
    "        return self\n",
    "\n",
    "    def get_action(self, key: jr.PRNGKey, observation):\n",
    "        return self.next.get_action(key, observation), self\n",
    "\n",
    "    def get_value(self, observation):\n",
    "        return self.next.get_value(observation), self\n",
    "\n",
    "    def config(self, **kwargs):\n",
    "        return self\n",
    "\n",
    "    def get_obs_size(self):\n",
    "        return self.next.obs_size\n",
    "\n",
    "    def get_act_size(self):\n",
    "        return self.next.act_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tHTxpOAzg-pO"
   },
   "source": [
    "Frequently the distribution of observations is quite \"dispersed\": it can have large values, be centered not around zero, and so on. While the MLP should still be able to learn it, it will take much more time to converge to the solution. Thus, it is common to normalize observations: collect running mean and std of all the observations observed so far, and then, before passing them to the model, normalize them, so that the distribution of the input to the network (hopefully) follows standard Gaussian distribution. This is done by **ObservationNormalizingWrapper**.\n",
    "\n",
    "This allows the underlying model outputs to be invariant to the shifting/scaling of the observation distribution, which is quite a nice property."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "mn_NOyjBv2ni",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.812980173Z",
     "start_time": "2023-12-07T15:17:10.368257019Z"
    }
   },
   "outputs": [],
   "source": [
    "class RunningStats(eqx.Module):\n",
    "    \"\"\"Stores/updates the parameters of the running distribution.\"\"\"\n",
    "\n",
    "    mean: jax.Array\n",
    "    M2: jax.Array  # sum of second moments of the samples (sum of variances)\n",
    "    n: jax.Array\n",
    "    size: int\n",
    "\n",
    "    # we are initializing n with two so that we don't get division by zero, ever\n",
    "    # this biases the running statistics, but not really that much\n",
    "    def __init__(self, size, mean=None, M2=None, n=jnp.int32(2)):\n",
    "        self.size = size\n",
    "        self.mean = (jnp.zeros(size)) if mean is None else mean\n",
    "        self.M2 = (jnp.zeros(size) + 1e-6) if M2 is None else M2\n",
    "        self.n = n\n",
    "\n",
    "    def process(self, obs):\n",
    "        std = jnp.sqrt(self.M2 / self.n)\n",
    "        std = eqx.error_if(\n",
    "            std,\n",
    "            std.shape != obs.shape,\n",
    "            \"Standard deviation should have the same shape as the observation, \"\n",
    "            + f\"std shape is {std.shape} but observation shape is {obs.shape}\",\n",
    "        )\n",
    "\n",
    "        # clip std, so that we don't get extreme values\n",
    "        std = jnp.clip(std, 1e-6, 1e6)\n",
    "\n",
    "        # clip the extreme outliers -> more stability during training.\n",
    "        # by Chebyshev inequality, ~99% of values are not clipped.\n",
    "        processed = jnp.clip((obs - self.mean) / std, -10, 10)\n",
    "\n",
    "        return processed, obs\n",
    "\n",
    "    def update_single(self, obs):\n",
    "        return self.update(obs[None, :])\n",
    "\n",
    "    def update(self, obs):\n",
    "        obs = eqx.error_if(\n",
    "            obs,\n",
    "            len(obs.shape) != 2 or obs.shape[1] != self.size,\n",
    "            f\"Batched observation should have the shape of (_, {self.size}),\"\n",
    "            + f\"but got {obs.shape}\",\n",
    "        )\n",
    "\n",
    "        n = self.n + obs.shape[0]\n",
    "\n",
    "        diff_to_old_mean = obs - self.mean\n",
    "        new_mean = self.mean + diff_to_old_mean.sum(axis=0) / n\n",
    "\n",
    "        diff_to_new_mean = obs - new_mean\n",
    "        var_upd = jnp.sum(diff_to_old_mean * diff_to_new_mean, axis=0)\n",
    "        M2 = self.M2 + var_upd\n",
    "\n",
    "        return RunningStats(self.size, mean=new_mean, M2=M2, n=n)\n",
    "\n",
    "\n",
    "class ObservationNormalizingWrapper(BaseWrapper):\n",
    "    \"\"\"Wrapper, that normalizes the observations during 'runtime'.\"\"\"\n",
    "\n",
    "    def __init__(self, next, params=None):\n",
    "        self.next = next\n",
    "        self.params = (\n",
    "            RunningStats(self.next.get_obs_size()) if params is None else params\n",
    "        )\n",
    "\n",
    "    def get_value(self, observation):\n",
    "        observation, updated_params = self.params.process(observation)\n",
    "        out, new_next = self.next.get_value(observation)\n",
    "        return out, ObservationNormalizingWrapper(new_next, updated_params)\n",
    "\n",
    "    def get_action(self, key: jr.PRNGKey, observation):\n",
    "        observation, updated_params = self.params.process(observation)\n",
    "        out, new_next = self.next.get_action(key, observation)\n",
    "        return out, ObservationNormalizingWrapper(new_next, updated_params)\n",
    "\n",
    "    def config(self, **kwargs):\n",
    "        params = self.params\n",
    "\n",
    "        if \"force_running_stats_update\" in kwargs:\n",
    "            params = self.params.update(kwargs.get(\"force_running_stats_update\"))\n",
    "\n",
    "        return ObservationNormalizingWrapper(self.next.config(**kwargs), params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r--AVKFcjCkV"
   },
   "source": [
    "When considering most of the environments, there are actions constraints that are present: if, for example, the action is the torque we apply, it cannot be very large, it is limited by physical properties of the system! But, since the values returned by **ActorCritic** has normal distribution, there is a non-zero probability of the sampled action to be larger than this \"physical\" limitation. To avoid this issue, we introduce action wrappers: both of them use a little bit different methods to constraint the output action norm.\n",
    "* **ActionTanhConstraintWrapper** is the most common action constraining method. It uses tanh as a \"smooth\" constraint, and then rescales it to the desired range. This method is nice, because there are non-zero gradients everywhere on the real domain, so the gradient descent have some idea in which direction is \"descent\".\n",
    "* **ActionExactConstraintWrapper** is the clipping action constraint. It simply clips the output action of the previous wrapper to the given range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dN2UM0U20IMU",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.818035397Z",
     "start_time": "2023-12-07T15:17:11.816042106Z"
    }
   },
   "outputs": [],
   "source": [
    "class ActionTanhConstraintWrapper(BaseWrapper):\n",
    "    \"\"\"'Softly' constraints an action to the provided range.\"\"\"\n",
    "\n",
    "    def __init__(self, next, range_low=-1.0, range_high=1.0):\n",
    "        self.next = next\n",
    "        self.params = ValueRange(range_low, range_high)\n",
    "\n",
    "    def get_action(self, key: jr.PRNGKey, observation: jax.Array):\n",
    "        action, new_next = self.next.get_action(key, observation)\n",
    "\n",
    "        scale = self.params.high - self.params.low\n",
    "        offset = self.params.low\n",
    "        normalizing_function = lambda x: (jnp.tanh(x) / 2.0 + 0.5) * scale + offset\n",
    "        action = action.postprocess(normalizing_function)\n",
    "\n",
    "        return action, self.set_next(new_next)\n",
    "\n",
    "\n",
    "class ActionExactConstraintWrapper(BaseWrapper):\n",
    "    \"\"\"Clips an action to the given range.\"\"\"\n",
    "\n",
    "    def __init__(self, next, range_low, range_high):\n",
    "        self.next = next\n",
    "        self.params = ValueRange(range_low, range_high)\n",
    "\n",
    "    def get_action(self, key: jr.PRNGKey, observation):\n",
    "        action, new_next = self.next.get_action(key, observation)\n",
    "        action = action.postprocess(\n",
    "            lambda x: jnp.clip(x, self.range_low, self.range_high)\n",
    "        )\n",
    "        return action, self.set_next(new_next)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mpRwALv9tKaj"
   },
   "source": [
    "The next two functions are needed to collect trajectories from the environment, provided \"policy\": a function that given an observation, returns the **Action** PyTree.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "w8HiREHa-2ZF",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.823452888Z",
     "start_time": "2023-12-07T15:17:11.819722694Z"
    }
   },
   "outputs": [],
   "source": [
    "def actor_step(key: jr.PRNGKey, env, env_state, policy: Callable, extra_fields):\n",
    "    \"\"\"Makes a single step with the provided policy in the environment.\"\"\"\n",
    "    keys_policy = jr.split(key, env_state.obs.shape[0])\n",
    "    action, _ = eqx.filter_vmap(policy)(keys_policy, env_state.obs)\n",
    "    next_state = env.step(env_state, action.transformed)\n",
    "\n",
    "    return next_state, Transition(\n",
    "        observation=env_state.obs,\n",
    "        action=action,\n",
    "        reward=next_state.reward,\n",
    "        next_observation=next_state.obs,\n",
    "        # extract requested additional fields\n",
    "        extras={x: next_state.info[x] for x in extra_fields},\n",
    "    )\n",
    "\n",
    "\n",
    "def generate_unroll(\n",
    "    key: jr.PRNGKey, env, env_state, policy: Callable, unroll_length, extra_fields\n",
    "):\n",
    "    \"\"\"Collects trajectories of given unroll length.\"\"\"\n",
    "\n",
    "    def f(carry, _):\n",
    "        current_key, state = carry\n",
    "        current_key, next_key = jr.split(current_key)\n",
    "\n",
    "        next_state, transition = actor_step(\n",
    "            current_key, env, state, policy, extra_fields=extra_fields\n",
    "        )\n",
    "        return (next_key, next_state), transition\n",
    "\n",
    "    (_, final_state), data = filter_scan(f, (key, env_state), (), length=unroll_length)\n",
    "    return final_state, data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "He6gadXhuSQZ"
   },
   "source": [
    "The **Evaluator** evaluates a given agent on the environment, providing quite a bit of interesting information about the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "T-7APS0ltH8j",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898150197Z",
     "start_time": "2023-12-07T15:17:11.825789403Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluates agent on the environment.\n",
    "    It is not jittable, since run_evaluation needs time.time()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, agent, num_eval_envs, episode_length):\n",
    "        self._eval_walltime = 0.0\n",
    "        self.eval_env = envs.training.EvalWrapper(eval_env)\n",
    "        self.episode_length = episode_length\n",
    "        self.num_eval_envs = num_eval_envs\n",
    "        self._steps_per_unroll = episode_length * num_eval_envs\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def evaluate(self, key: jr.PRNGKey, agent):\n",
    "        reset_keys = jr.split(key, self.num_eval_envs)\n",
    "        eval_first_state = self.eval_env.reset(reset_keys)\n",
    "        return generate_unroll(\n",
    "            key,\n",
    "            self.eval_env,\n",
    "            eval_first_state,\n",
    "            agent.get_action,\n",
    "            unroll_length=self.episode_length,\n",
    "            extra_fields=(\"truncation\",),\n",
    "        )[0]\n",
    "\n",
    "    def run_evaluation(\n",
    "        self, key: jr.PRNGKey, agent, training_metrics, aggregate_episodes: bool = True\n",
    "    ):\n",
    "        t = time.time()\n",
    "        eval_state = self.evaluate(key, agent)\n",
    "        eval_metrics = eval_state.info[\"eval_metrics\"]\n",
    "        eval_metrics.active_episodes.block_until_ready()\n",
    "        epoch_eval_time = time.time() - t\n",
    "        metrics = {}\n",
    "        for fn in [jnp.mean, jnp.std]:\n",
    "            suffix = \"_std\" if fn == jnp.std else \"\"\n",
    "            metrics.update(\n",
    "                {\n",
    "                    f\"eval/episode_{name}{suffix}\": (\n",
    "                        fn(value) if aggregate_episodes else value\n",
    "                    )\n",
    "                    for name, value in eval_metrics.episode_metrics.items()\n",
    "                }\n",
    "            )\n",
    "        metrics[\"eval/avg_episode_length\"] = jnp.mean(eval_metrics.episode_steps)\n",
    "        metrics[\"eval/epoch_eval_time\"] = epoch_eval_time\n",
    "        metrics[\"eval/sps\"] = self._steps_per_unroll / epoch_eval_time\n",
    "        self._eval_walltime = self._eval_walltime + epoch_eval_time\n",
    "        metrics = {\"eval/walltime\": self._eval_walltime, **training_metrics, **metrics}\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PQwtzmYhuwx7"
   },
   "source": [
    "The next functions are the core of the PPO algorithm: loss computation, and generalized advantage estimation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "vZK-JSgnqMnJ",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898422241Z",
     "start_time": "2023-12-07T15:17:11.866094410Z"
    }
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def compute_gae(\n",
    "    truncation, rewards, values, bootstrap_value, gae_lambda, time_discount\n",
    "):\n",
    "    \"\"\"\n",
    "    Computes Generalized Advantage Estimation (GAE).\n",
    "    https://arxiv.org/abs/1506.02438 (formula 16)\n",
    "    \"\"\"\n",
    "\n",
    "    def to_scan(gae_t_plus_one, inputs):\n",
    "        delta_t, truncation_t = inputs\n",
    "        gae_t = gae_t_plus_one * time_discount * gae_lambda + delta_t\n",
    "        gae_t = gae_t * (1 - truncation_t)\n",
    "        return gae_t, gae_t\n",
    "\n",
    "    next_values = jnp.concatenate([values[1:], bootstrap_value], axis=0)\n",
    "    deltas = rewards + time_discount * next_values - values\n",
    "    _, advantages = lax.scan(to_scan, 0.0, (deltas, truncation), reverse=True)\n",
    "    return advantages + values, advantages\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def compute_loss(key: jr.PRNGKey, data, agent, params):\n",
    "    \"\"\"\n",
    "    Computes standard PPO loss on a single trajectory, with clipped surrogate objective.\n",
    "    https://arxiv.org/abs/1707.06347 (link to the PPO paper)\n",
    "    \"\"\"\n",
    "    key_actions = jr.split(key, data.observation.shape[0])\n",
    "\n",
    "    # the second returned value is updated agent (stacked unroll_length times)\n",
    "    new_actions, _ = eqx.filter_vmap(agent.get_action)(key_actions, data.observation)\n",
    "    baseline, _ = eqx.filter_vmap(agent.get_value)(data.observation)\n",
    "\n",
    "    baseline = eqx.error_if(\n",
    "        baseline,\n",
    "        baseline.shape != (params.unroll_length, 1),\n",
    "        f\"Baseline Values should have shape {(params.unroll_length, 1)}, \"\n",
    "        + f\"but got {baseline.shape}\",\n",
    "    )\n",
    "\n",
    "    # (unroll_length, 1) -> (unroll_length,)\n",
    "    baseline = baseline.reshape((params.unroll_length,))\n",
    "    bootstrap_value, _ = agent.get_value(data.next_observation[-1])\n",
    "\n",
    "    rewards = data.reward * params.reward_scaling\n",
    "    behaviour_actions = data.action\n",
    "\n",
    "    get_log_pdf_from_distr = lambda action, distr: distr.get_pdf(action)\n",
    "\n",
    "    # compute log of pdfs of the the old actions for new/old distributions\n",
    "    # we use \"raw\" actions, since they are directly sampled from the distributions\n",
    "    new_distr_log_pdf = eqx.filter_vmap(get_log_pdf_from_distr)(\n",
    "        behaviour_actions.raw, new_actions.distr\n",
    "    )\n",
    "    old_distr_log_pdf = eqx.filter_vmap(get_log_pdf_from_distr)(\n",
    "        behaviour_actions.raw, behaviour_actions.distr\n",
    "    )\n",
    "    # ratio of probabilities that the old (behavioural) action will be taken\n",
    "    rho = jnp.exp(new_distr_log_pdf - old_distr_log_pdf)\n",
    "\n",
    "    target_values, advantages = compute_gae(\n",
    "        truncation=data.extras[\"truncation\"],\n",
    "        rewards=rewards,\n",
    "        values=baseline,\n",
    "        bootstrap_value=bootstrap_value,\n",
    "        gae_lambda=params.gae_lambda,\n",
    "        time_discount=params.discounting,\n",
    "    )\n",
    "\n",
    "    # stop gradients for numerical stability, and since they are \"meaningless\"\n",
    "    # the point is that we train with respect to these parameters,\n",
    "    # and allowing them too to be trainable too leads to weird results\n",
    "    target_values = lax.stop_gradient(target_values)\n",
    "    advantages = lax.stop_gradient(advantages)\n",
    "\n",
    "    # compute clipped policy loss\n",
    "    surrogate_loss1 = rho * advantages\n",
    "    surrogate_loss2 = (\n",
    "        jnp.clip(rho, 1 - params.clipping_epsilon, 1 + params.clipping_epsilon)\n",
    "        * advantages\n",
    "    )\n",
    "    policy_loss = -jnp.mean(jnp.minimum(surrogate_loss1, surrogate_loss2))\n",
    "\n",
    "    # compute value loss\n",
    "    v_error = target_values - baseline\n",
    "    v_loss = jnp.mean(v_error * v_error) * params.value_loss_factor\n",
    "\n",
    "    # and finally, the entropy loss (we encourage higher entropy)\n",
    "    entropy = jnp.mean(new_actions.distr.entropy())\n",
    "    entropy_loss = params.entropy_cost * -entropy\n",
    "\n",
    "    # sum all the losses up\n",
    "    total_loss = policy_loss + v_loss + entropy_loss\n",
    "    return total_loss, {\n",
    "        \"total_loss\": total_loss,\n",
    "        \"rho\": rho,\n",
    "        \"p_loss\": policy_loss,\n",
    "        \"v_loss\": v_loss,\n",
    "        \"entropy_loss\": entropy_loss,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YNgDezLArFGy"
   },
   "source": [
    "The **sgd_step** does not do a single SGD step. It does a bunch of optimizer steps on the minibatches.\n",
    "\n",
    "Most importantly, there are tree things that this function does:\n",
    "\n",
    "1. The passed data is shuffled, and partitioned into minibatches.\n",
    "2. The mean loss over each of the minibatches is computed.\n",
    "3. Using reverse autodiff (**eqx.filter_value_and_grad**), the gradients are computed, and we update the agent.\n",
    "\n",
    "It is important to note that we collect some metrics (a dictionary with some info) throughout training, that are averaged for every SGD step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "cJunuMFL2sfm",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898529950Z",
     "start_time": "2023-12-07T15:17:11.866350615Z"
    }
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def clip_by_norm(x, max_norm=1.0):\n",
    "    \"\"\"Clips the norm of the vector, with some whistles\"\"\"\n",
    "    max_norm = eqx.error_if(max_norm, max_norm < 0, \"Clip norm should be non-negative\")\n",
    "    norm = lax.cond(\n",
    "        jnp.array_equal(x, jnp.zeros_like(x)),\n",
    "        lambda *_: jnp.float32(1.0),\n",
    "        lambda *_: jnp.linalg.norm(x),\n",
    "    )\n",
    "    return x * jnp.minimum(max_norm / norm, 1.0)\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def sgd_step(key: jr.PRNGKey, optimizer, agent, data, params):\n",
    "    key_perm, key_grad = jr.split(key)\n",
    "\n",
    "    def convert_data(x: jax.Array):\n",
    "        \"\"\"Shuffles input data, and partitions in into the minibatches\"\"\"\n",
    "        x = jr.permutation(key_perm, x)\n",
    "        x = jnp.reshape(x, (params.num_minibatches, -1) + x.shape[1:])\n",
    "        return x\n",
    "\n",
    "    shuffled_data = jtu.tree_map(convert_data, data)\n",
    "\n",
    "    # check that the shapetor is correct\n",
    "    desired_shape = (params.num_minibatches, params.batch_size, params.unroll_length)\n",
    "    shuffled_data = eqx.error_if(\n",
    "        shuffled_data,\n",
    "        shuffled_data.observation.shape[:-1] != desired_shape,\n",
    "        f\"Minibatch data shape is wrong, should be {desired_shape} \"\n",
    "        + f\"but was {shuffled_data.observation.shape[:-1]}\",\n",
    "    )\n",
    "\n",
    "    def minibatch_step_to_scan(carry, data):\n",
    "        key, optimizer, agent = carry\n",
    "        key_next, key_loss = jr.split(key)\n",
    "\n",
    "        def batched_loss(agent, data):\n",
    "            loss_f = lambda data, agent: compute_loss(key_loss, data, agent, params)\n",
    "            loss_value, metrics = eqx.filter_vmap(loss_f, in_axes=(0, None))(\n",
    "                data, agent\n",
    "            )\n",
    "            # we waste less memory by computing mean for everything in metrics\n",
    "            return loss_value.mean(), jtu.tree_map(lambda x: x.mean(axis=0), metrics)\n",
    "\n",
    "        get_value_and_grad = eqx.filter_value_and_grad(\n",
    "            jtu.Partial(batched_loss, data=data), has_aux=True\n",
    "        )\n",
    "        (loss, metrics), grads = get_value_and_grad(agent)\n",
    "\n",
    "        # gradient clipping -> more stable training\n",
    "        def filter_and_clip_grads(grad, trainable):\n",
    "            # making sure that we update only trainable stuff\n",
    "            # since grads are also computed for all the wrappers params\n",
    "            # which breaks optax\n",
    "            if grad is None or trainable is None:\n",
    "                return None\n",
    "            return clip_by_norm(grad, params.max_gradient_norm)\n",
    "\n",
    "        grads = jtu.tree_map(filter_and_clip_grads, grads, agent.get_trainable())\n",
    "\n",
    "        updates, new_optimizer = optimizer.update(grads)\n",
    "        new_agent = eqx.apply_updates(agent, updates)\n",
    "\n",
    "        return (key_next, new_optimizer, new_agent), metrics\n",
    "\n",
    "    (_, new_optimizer, new_agent), metrics = filter_scan(\n",
    "        minibatch_step_to_scan,\n",
    "        (key_grad, optimizer, agent),\n",
    "        shuffled_data,\n",
    "        length=params.num_minibatches,\n",
    "    )\n",
    "\n",
    "    return new_optimizer, new_agent, jtu.tree_map(lambda x: x.mean(axis=0), metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wTzKcpE7nT-S"
   },
   "source": [
    "**training_step** is quite a long function, but what it does is quite simple.\n",
    "\n",
    "It consists of three meaningful parts:\n",
    "* Collection of **batch_size** number of trajectories of **unroll_length** length from the provided environment, with the current policy. The agent remains constant throughout the collection of the trajectory.\n",
    "* Split of the collected data into a convenient shape, which is (a lot, **unroll_length**), so that later we can vmap or map over the zero-th axis, and the mapped function will get just a single trajectory.\n",
    "* Update of the agent. We do an optimizer step on the computed PPO loss, and then we update the observation normalizing wrapper with just collected trajectories data. The running statistics is updated in a batch since it is easier, and it does not hurt performance much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "9gBOauWYRThH",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898624799Z",
     "start_time": "2023-12-07T15:17:11.866622128Z"
    }
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def training_step(carry, _, params):\n",
    "    key, training_state = carry\n",
    "    key_sgd, key_generate_unroll, key_next = jr.split(key, 3)\n",
    "\n",
    "    agent = training_state.agent\n",
    "    env = training_state.env\n",
    "\n",
    "    # wrap the functions, so that lax.scan can use them\n",
    "    def unroll_to_scan(carry, _):\n",
    "        key_unroll, state, policy = carry\n",
    "        key_unroll, next_key = jr.split(key_unroll)\n",
    "\n",
    "        next_state, generated = generate_unroll(\n",
    "            key_unroll,\n",
    "            env.env,\n",
    "            state,\n",
    "            policy,\n",
    "            params.unroll_length,\n",
    "            extra_fields=(\"truncation\",)\n",
    "            # truncation is the signal that environment evaluation has finished\n",
    "            # in this case we 'reset' the advantage computation\n",
    "        )\n",
    "        return (next_key, next_state, policy), generated\n",
    "\n",
    "    def sgd_step_to_scan(carry, _):\n",
    "        key_sgd, optimizer, agent = carry\n",
    "        key_sgd, next_key = jr.split(key_sgd)\n",
    "\n",
    "        new_opt, new_ag, metrics = sgd_step(key_sgd, optimizer, agent, data, params)\n",
    "        return (next_key, new_opt, new_ag), metrics\n",
    "\n",
    "    # generate unroll with the current policy\n",
    "    (_, new_env_state, *_), data = filter_scan(\n",
    "        unroll_to_scan,\n",
    "        (key_generate_unroll, env.state, agent.get_action),\n",
    "        (),\n",
    "        length=params.num_minibatches,\n",
    "    )\n",
    "\n",
    "    # transform all the data from the unroll into more convenient shape\n",
    "    data = jtu.tree_map(lambda x: jnp.swapaxes(x, 1, 2), data)\n",
    "    data = jtu.tree_map(lambda x: jnp.reshape(x, (-1,) + x.shape[2:]), data)\n",
    "\n",
    "    # check the correctness of the shape\n",
    "    target_data_shape = (\n",
    "        params.batch_size * params.num_minibatches,\n",
    "        params.unroll_length,\n",
    "        params.env.observation_size,\n",
    "    )\n",
    "    data = eqx.error_if(\n",
    "        data,\n",
    "        data.observation.shape != target_data_shape,\n",
    "        f\"Reshaped unroll data (observation) must have shape of \"\n",
    "        f\"{target_data_shape} but had {data.observation.shape}\",\n",
    "    )\n",
    "\n",
    "    # optimize the model, do a few optimizer steps\n",
    "    (_, new_optimizer, new_agent), metrics = filter_scan(\n",
    "        sgd_step_to_scan,\n",
    "        (key_sgd, training_state.optimizer, agent),\n",
    "        (),\n",
    "        length=params.num_updates_per_batch,\n",
    "    )\n",
    "\n",
    "    # update the normalizing wrapper with collected observations\n",
    "    reshaped_data = data.observation.reshape(-1, data.observation.shape[-1])\n",
    "    new_agent = new_agent.config(force_running_stats_update=reshaped_data)\n",
    "\n",
    "    # the first training iteration used only to update observation normalizing wrapper\n",
    "    # otherwise the first step would be random\n",
    "    get_new = lambda *_: (new_optimizer, new_agent)\n",
    "    get_old = lambda *_: (training_state.optimizer, training_state.agent)\n",
    "    new_optimizer, new_agent = filter_cond(env.steps_done != 0, get_new, get_old)\n",
    "\n",
    "    env_steps_made = params.batch_size * params.num_minibatches * params.unroll_length\n",
    "\n",
    "    # construct new training state, with all the updated stuff\n",
    "    new_training_state = TrainingState(\n",
    "        optimizer=new_optimizer,\n",
    "        agent=new_agent,\n",
    "        env=Environment(env.env, new_env_state, env.steps_done + env_steps_made),\n",
    "    )\n",
    "    return (key_next, new_training_state), metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6C2Z4QV1m8R8"
   },
   "source": [
    "**training_epoch** not only runs a bunch of **training_steps**, but it also figures out the number of **training_steps** to run judging by the required number of timesteps that we want to train for, and other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "tQTzozRx_JET",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898736593Z",
     "start_time": "2023-12-07T15:17:11.868638374Z"
    }
   },
   "outputs": [],
   "source": [
    "@eqx.filter_jit\n",
    "def training_epoch(key: jr.PRNGKey, training_state, params):\n",
    "    env_step_per_training_step = (\n",
    "        params.batch_size * params.unroll_length * params.num_minibatches\n",
    "    )\n",
    "    num_evals_after_init = params.num_evals - 1 if params.num_evals > 1 else 1\n",
    "    num_training_steps_per_epoch = 1 + params.num_timesteps // (\n",
    "        num_evals_after_init * env_step_per_training_step\n",
    "    )\n",
    "\n",
    "    (_, training_state), metrics = filter_scan(\n",
    "        jtu.Partial(training_step, params=params),\n",
    "        (key, training_state),\n",
    "        (),\n",
    "        length=num_training_steps_per_epoch,\n",
    "    )\n",
    "\n",
    "    return training_state, metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M3g_GuWhqFbW"
   },
   "source": [
    "We can define a dataclass, that contains all the parameters of the training algorithm. This class allows to introduce new parameters easily, change (some) parameters without JAX triggering retracing of everything, and overall it is much more convenient to pass them as an argument to a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "mNi-B8el_gAR",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898827751Z",
     "start_time": "2023-12-07T15:17:11.893100255Z"
    }
   },
   "outputs": [],
   "source": [
    "class HyperParameters(eqx.Module):\n",
    "    \"\"\"All the parameters for the algorithm you will ever need.\"\"\"\n",
    "\n",
    "    # parameters that are forcefully static, and you are not allowed to change them,\n",
    "    # unless you really want to retrace most of the jitted functions\n",
    "    env: envs.Env = eqx.field(static=True)\n",
    "    episode_length: int = eqx.field(static=True)\n",
    "\n",
    "    num_timesteps: int = eqx.field(default=30_000_000, static=True)\n",
    "    seed: int = eqx.field(default=0, static=True)\n",
    "    num_evals: int = eqx.field(default=10, static=True)\n",
    "\n",
    "    # parameters that are 'changeable' throughout training.\n",
    "    # Does not mean that you should change them :)\n",
    "    learning_rate: float = 1e-4\n",
    "    clipping_epsilon: float = 0.2\n",
    "    batch_size: int = 32\n",
    "    eval_batch_size: int = 16\n",
    "    entropy_cost: float = 0\n",
    "    discounting: float = 0.99\n",
    "    gae_lambda: float = 0.95\n",
    "    num_updates_per_batch: int = 2\n",
    "    num_minibatches: int = 16\n",
    "    unroll_length: int = 10\n",
    "    reward_scaling: float = 1.0\n",
    "    max_gradient_norm: float = 0.5\n",
    "    value_loss_factor: float = 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tvsjfBo4knPa"
   },
   "source": [
    "Define some other utility dataclasses.\n",
    "\n",
    "Besides, passing a single PyTree to the function is much nicer than passing 10 millions variables, isn't it?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "BWIacWU6kmmb",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.898955095Z",
     "start_time": "2023-12-07T15:17:11.893368875Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transition(eqx.Module):\n",
    "    \"\"\"Represents a transition between two adjacent environment states.\"\"\"\n",
    "\n",
    "    observation: jax.Array  # observation on the current state\n",
    "    action: Action  # action that was taken on the current state\n",
    "    reward: float  # reward, that was given as the result of the action\n",
    "    next_observation: jax.Array  # next observation\n",
    "    extras: dict  # any simulator-extracted hints, like end of episode signal\n",
    "\n",
    "\n",
    "class Optimizer(eqx.Module):\n",
    "    \"\"\"An optax optimizer wrapped with its state together.\"\"\"\n",
    "\n",
    "    optimizer: optax.GradientTransformation = eqx.field(static=True)\n",
    "    state: optax.OptState\n",
    "\n",
    "    def update(self, grads):\n",
    "        out_updates, new_state = self.optimizer.update(grads, self.state)\n",
    "        return out_updates, Optimizer(self.optimizer, new_state)\n",
    "\n",
    "\n",
    "class Environment(eqx.Module):\n",
    "    \"\"\"A Brax environment, wrapped with its state and step counter together.\"\"\"\n",
    "\n",
    "    env: envs.base.Env = eqx.field(static=True)\n",
    "    state: envs.base.State\n",
    "    steps_done: jax.Array = eqx.field(default=0, converter=jnp.asarray)\n",
    "\n",
    "\n",
    "class TrainingState(eqx.Module):\n",
    "    optimizer: Optimizer\n",
    "    agent: Agent\n",
    "    env: Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTXzeqZJk97i"
   },
   "source": [
    "The main **train** function is quite simple, just initialize some brax-wrapped environments, some variables, some states, and then just run **training_epoch** a few times.\n",
    "\n",
    "**train** is not JIT-traced, so that user-defined **progress** can do anything: printing something, logging to wandb, plotting graphs, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "xs87w2hQKDAn",
    "ExecuteTime": {
     "end_time": "2023-12-07T15:17:11.899061126Z",
     "start_time": "2023-12-07T15:17:11.893532903Z"
    }
   },
   "outputs": [],
   "source": [
    "def train(agent, params, progress=lambda *_: None):\n",
    "    key = jr.PRNGKey(params.seed)\n",
    "    key_local, key_env = jr.split(key, 2)\n",
    "\n",
    "    env = envs.training.wrap(params.env, episode_length=params.episode_length)\n",
    "    reset_fn = jax.jit(env.reset)\n",
    "\n",
    "    # we are using Adam, since, well, everybody uses Adam\n",
    "    optimizer = optax.adam(learning_rate=params.learning_rate)\n",
    "    trainable_agent_arrays = eqx.filter(agent.get_trainable(), eqx.is_array)\n",
    "    key_envs = jr.split(key_env, params.batch_size)\n",
    "\n",
    "    training_state = TrainingState(\n",
    "        optimizer=Optimizer(optimizer, optimizer.init(trainable_agent_arrays)),\n",
    "        agent=agent,\n",
    "        env=Environment(env, reset_fn(key_envs), steps_done=0),\n",
    "    )\n",
    "\n",
    "    evaluator = Evaluator(\n",
    "        env,\n",
    "        agent,\n",
    "        num_eval_envs=params.eval_batch_size,\n",
    "        episode_length=params.episode_length,\n",
    "    )\n",
    "\n",
    "    # run very first eval before any training\n",
    "    metrics = {}\n",
    "    if params.num_evals > 1:\n",
    "        metrics = evaluator.run_evaluation(\n",
    "            key, training_state.agent, training_metrics={}\n",
    "        )\n",
    "        progress(0, metrics)\n",
    "\n",
    "    num_of_epochs = max(params.num_evals - 1, 1)\n",
    "    for it in range(num_of_epochs):\n",
    "        # update the keys\n",
    "        key_epoch, key_local, key_eval = jr.split(key_local, 3)\n",
    "\n",
    "        # train\n",
    "        training_state, training_metrics = training_epoch(\n",
    "            key_epoch, training_state, params\n",
    "        )\n",
    "\n",
    "        # reset the environment state\n",
    "        key_envs = jr.split(key_local, params.batch_size)\n",
    "        training_state = eqx.tree_at(\n",
    "            where=lambda t: t.env.state,\n",
    "            pytree=training_state,\n",
    "            replace=reset_fn(key_envs),\n",
    "        )\n",
    "\n",
    "        # update metrics\n",
    "        metrics = evaluator.run_evaluation(\n",
    "            key_eval, training_state.agent, training_metrics\n",
    "        )\n",
    "        progress(training_state.env.steps_done, metrics)\n",
    "\n",
    "    return (training_state.agent, metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiJVXNk4l6bv"
   },
   "source": [
    "Now, just initialize the environment, agent, and run the train function!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 787
    },
    "id": "76O4DFbHNDL1",
    "outputId": "073a6544-d350-43df-dcc0-db07345b8648"
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<Figure size 640x480 with 1 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training (and tracing) done in 201.78 seconds\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt #Alright, I lied, we also need matplotlib as the dependency...\n",
    "from IPython.display import clear_output\n",
    "from dill import dump, load\n",
    "\n",
    "env = envs.create(env_name=\"ant\", backend=\"spring\")\n",
    "agent = Agent(jr.PRNGKey(42), env.observation_size, env.action_size)\n",
    "agent = ObservationNormalizingWrapper(agent)\n",
    "agent = ActionTanhConstraintWrapper(agent, range_low=-1.0, range_high=1.0)\n",
    "\n",
    "xdata, ydata = [], []\n",
    "\n",
    "def progress(step_num, metrics):\n",
    "  xdata.append(step_num)\n",
    "  ydata.append(metrics[\"eval/episode_reward\"])\n",
    "  clear_output(wait=True)\n",
    "\n",
    "  plt.xlim([0, 40_000_000])\n",
    "  plt.ylim([0, 4000])\n",
    "  plt.xlabel(\"# environment steps\")\n",
    "  plt.ylabel(\"reward per episode\")\n",
    "\n",
    "  plt.plot(xdata, ydata, \"b\")\n",
    "  plt.show()\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "agent, metrics = train(\n",
    "  agent,\n",
    "  params=HyperParameters(\n",
    "      env=env,\n",
    "      num_timesteps=40_000_000,\n",
    "      num_evals=10,\n",
    "      reward_scaling=0.1,\n",
    "      episode_length=1000,\n",
    "      unroll_length=5,\n",
    "      num_minibatches=32,\n",
    "      num_updates_per_batch=4,\n",
    "      discounting=0.99,\n",
    "      learning_rate=3e-4,\n",
    "      entropy_cost=1e-3,\n",
    "      batch_size=1024,\n",
    "      eval_batch_size=256,\n",
    "      gae_lambda=0.97,\n",
    "      max_gradient_norm=1.0,\n",
    "      clipping_epsilon=0.3,\n",
    "  ),\n",
    "  progress=progress,\n",
    ")\n",
    "\n",
    "print(f\"Training (and tracing) done in {time.time() - t0:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "class GHyps(eqx.Module):\n",
    "    "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "env = envs.create(env_name=\"ant\", backend=\"spring\")\n",
    "agent = Agent(jr.PRNGKey(42), env.observation_size, env.action_size)\n",
    "agent = ObservationNormalizingWrapper(agent)\n",
    "agent = ActionTanhConstraintWrapper(agent, range_low=-1.0, range_high=1.0)\n",
    "\n",
    "xdata, ydata = [], []\n",
    "\n",
    "def progress(step_num, metrics):\n",
    "    xdata.append(step_num)\n",
    "    ydata.append(metrics[\"eval/episode_reward\"])\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    plt.xlim([0, 40_000_000])\n",
    "    plt.ylim([0, 4000])\n",
    "    plt.xlabel(\"# environment steps\")\n",
    "    plt.ylabel(\"reward per episode\")\n",
    "\n",
    "    plt.plot(xdata, ydata, \"b\")\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "t0 = time.time()\n",
    "\n",
    "agent, metrics = train(\n",
    "    agent,\n",
    "    params=HyperParameters(\n",
    "        env=env,\n",
    "        num_timesteps=40_000_000,\n",
    "        num_evals=10,\n",
    "        reward_scaling=0.1,\n",
    "        episode_length=1000,\n",
    "        unroll_length=5,\n",
    "        num_minibatches=32,\n",
    "        num_updates_per_batch=4,\n",
    "        discounting=0.99,\n",
    "        learning_rate=3e-4,\n",
    "        entropy_cost=1e-3,\n",
    "        batch_size=1024,\n",
    "        eval_batch_size=256,\n",
    "        gae_lambda=0.97,\n",
    "        max_gradient_norm=1.0,\n",
    "        clipping_epsilon=0.3,\n",
    "    ),\n",
    "    progress=progress,\n",
    ")\n",
    "\n",
    "print(f\"Training (and tracing) done in {time.time() - t0:.2f} seconds\")\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from tqdm import tqdm as tqdm"
   ],
   "metadata": {
    "id": "VaGLUBqW6btq",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:15.533615286Z",
     "start_time": "2023-11-26T14:18:15.472155260Z"
    }
   },
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "agent_r = Agent(jr.PRNGKey(42), env.observation_size, env.action_size)\n",
    "agent_r = ObservationNormalizingWrapper(agent_r)\n",
    "agent_r = ActionTanhConstraintWrapper(agent_r, range_low=-1.0, range_high=1.0)"
   ],
   "metadata": {
    "id": "n163hbKqnHAM",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:15.978790411Z",
     "start_time": "2023-11-26T14:18:15.477925704Z"
    }
   },
   "execution_count": 19,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now, lets look at the gradients of the simulation for different numbers of steps"
   ],
   "metadata": {
    "id": "T72SB7vCHRWT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "env = envs.create(env_name=\"ant\", backend=\"spring\")\n",
    "episode_length = 1000\n",
    "env = envs.training.wrap(env, episode_length=episode_length)\n",
    "env = envs.training.EvalWrapper(env)\n",
    "\n",
    "def actor_step_gradded(key: jr.PRNGKey, env, env_state, policy: Callable, extra_fields):\n",
    "    \"\"\"Makes a single step with the provided policy in the environment.\"\"\"\n",
    "    keys_policy = jr.split(key, env_state.obs.shape[0])\n",
    "    action, _ = eqx.filter_vmap(policy)(keys_policy, env_state.obs)\n",
    "    print(env_state.info)\n",
    "    next_state = env.step(env_state, action.transformed)\n",
    "\n",
    "    return next_state, Transition(\n",
    "        observation=env_state.obs,\n",
    "        action=action,\n",
    "        reward=next_state.reward,\n",
    "        next_observation=next_state.obs,\n",
    "        # extract requested additional fields\n",
    "        extras={x: next_state.info[x] for x in extra_fields},\n",
    "    )\n",
    "\n",
    "def generate_unroll_gradded(\n",
    "    key: jr.PRNGKey, env, env_state, policy: Callable, gradded_policy, unroll_length, extra_fields\n",
    "):\n",
    "    \"\"\"Collects trajectories of given unroll length.\"\"\"\n",
    "\n",
    "    def f(carry, _):\n",
    "        current_key, state = carry\n",
    "        current_key, next_key = jr.split(current_key)\n",
    "\n",
    "        next_state, transition = actor_step_gradded(\n",
    "            current_key, env, state, policy, extra_fields=extra_fields\n",
    "        )\n",
    "        return (next_key, next_state), transition\n",
    "    gradded_env_state, _transition = actor_step_gradded(key, env, env_state, gradded_policy, extra_fields)\n",
    "    (_, final_state), data = filter_scan(f, (key, gradded_env_state), (), length=unroll_length-1)\n",
    "    return final_state, data\n",
    "\n",
    "@eqx.filter_jit\n",
    "def eval(delta, env, agent, key, N=32, episode_length=1000):\n",
    "  old_value = agent.next.next.next.actor.mean_network.structure[0].weight\n",
    "  zeroholder = jnp.zeros_like(agent.next.next.next.actor.mean_network.structure[0].weight)\n",
    "  zeroholder = zeroholder.at[5,7].set(delta)\n",
    "  new_agent = eqx.tree_at(where=lambda a: a.next.next.next.actor.mean_network.structure[0].weight, pytree=agent, replace=old_value + zeroholder)\n",
    "  reset_keys = jr.split(key, N)\n",
    "  eval_first_state = env.reset(reset_keys)\n",
    "  return generate_unroll_gradded(\n",
    "      key,\n",
    "      env,\n",
    "      eval_first_state,\n",
    "      agent.get_action,\n",
    "      new_agent.get_action,\n",
    "      unroll_length=episode_length,\n",
    "      extra_fields=(\"truncation\",),\n",
    "  )[0].info['eval_metrics'].episode_metrics['reward'].mean()\n"
   ],
   "metadata": {
    "id": "pvnCJEUFHQt1",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:16.020494429Z",
     "start_time": "2023-11-26T14:18:15.983273420Z"
    }
   },
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "  def generate_unroll_mag(\n",
    "    key: jr.PRNGKey, env, env_state, agent, deltas, unroll_length, extra_fields\n",
    "):\n",
    "    def f(carry, delta_i):\n",
    "        current_key, state = carry\n",
    "        current_key, next_key = jr.split(current_key)\n",
    "\n",
    "        old_value = agent.next.next.next.actor.mean_network.structure[0].weight\n",
    "        zeroholder = jnp.zeros_like(old_value)\n",
    "        zeroholder = zeroholder.at[5,7].set(delta_i)\n",
    "        new_agent = eqx.tree_at(where=lambda a: a.next.next.next.actor.mean_network.structure[0].weight, pytree=agent, replace=old_value + zeroholder)\n",
    "        next_state, transition = actor_step_gradded(\n",
    "            current_key, env, state, new_agent.get_action, extra_fields=extra_fields\n",
    "        )\n",
    "        return (next_key, next_state), transition\n",
    "    (_, final_state), data = filter_scan(f, (key, env_state), deltas, length=unroll_length)\n",
    "    return final_state, data\n",
    "\n",
    "@eqx.filter_jit\n",
    "def all_grads_eval(deltas, env, agent, key, N=8, episode_length=200):\n",
    "    reset_keys = jr.split(key, N)\n",
    "    eval_first_state = env.reset(reset_keys)\n",
    "    return generate_unroll_mag(\n",
    "        key,\n",
    "        env,\n",
    "        eval_first_state,\n",
    "        agent, deltas,\n",
    "        unroll_length=episode_length,\n",
    "        extra_fields=(\"truncation\",),\n",
    "    )[0].info['eval_metrics'].episode_metrics['reward'].mean()\n",
    "\n",
    "def generate_unroll_mag_lambdas(\n",
    "    key: jr.PRNGKey, env, env_state, agent, deltas, lambdas, unroll_length, extra_fields\n",
    "):\n",
    "    def f(carry, delta_i):\n",
    "        current_key, state = carry\n",
    "        current_key, next_key = jr.split(current_key)\n",
    "\n",
    "        old_value = agent.next.next.next.actor.mean_network.structure[0].weight\n",
    "        zeroholder = jnp.zeros_like(old_value)\n",
    "        zeroholder = zeroholder.at[5,7].set(delta_i)\n",
    "        new_agent = eqx.tree_at(where=lambda a: a.next.next.next.actor.mean_network.structure[0].weight, pytree=agent, replace=old_value + zeroholder)\n",
    "        next_state, transition = actor_step_gradded(\n",
    "            current_key, env, state, new_agent.get_action, extra_fields=extra_fields\n",
    "        )\n",
    "        return (next_key, next_state), transition\n",
    "    (_, final_state), data = filter_scan(f, (key, env_state), deltas, length=unroll_length)\n",
    "    return final_state, data\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def all_grads_eval_lambdas(deltas, env, agent, key, lambdas=1.0, N=8, episode_length=200):\n",
    "    reset_keys = jr.split(key, N)\n",
    "    eval_first_state = env.reset(reset_keys)\n",
    "    rewards = generate_unroll_mag_lambdas(\n",
    "        key,\n",
    "        env,\n",
    "        eval_first_state,\n",
    "        agent, deltas, lambdas,\n",
    "        unroll_length=episode_length,\n",
    "        extra_fields=(\"truncation\",),\n",
    "    )\n",
    "    return jnp.sum(rewards[1].reward.mean(axis=1) * lambdas)"
   ],
   "metadata": {
    "id": "D3haNlA8_qHy",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:16.020932153Z",
     "start_time": "2023-11-26T14:18:16.014722532Z"
    }
   },
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_unroll_mag_pseudo(\n",
    "    key: jr.PRNGKey, env, env_state, agent, deltas, factor, unroll_length, extra_fields\n",
    "):\n",
    "    def f(carry, cons):\n",
    "        current_key, state = carry\n",
    "        current_key, next_key = jr.split(current_key)\n",
    "        delta_i, i = cons\n",
    "\n",
    "        old_value = agent.next.next.next.actor.mean_network.structure[0].weight\n",
    "        zeroholder = jnp.zeros_like(old_value)\n",
    "        zeroholder = zeroholder.at[5,7].set(delta_i * (factor ** (unroll_length - i)))\n",
    "        new_agent = eqx.tree_at(where=lambda a: a.next.next.next.actor.mean_network.structure[0].weight, pytree=agent, replace=old_value + zeroholder)\n",
    "        next_state, transition = actor_step_gradded(\n",
    "            current_key, env, state, new_agent.get_action, extra_fields=extra_fields\n",
    "        )\n",
    "        return (next_key, next_state), transition\n",
    "    (_, final_state), data = filter_scan(f, (key, env_state), (deltas, jnp.arange(len(deltas))), length=unroll_length)\n",
    "    return final_state, data\n",
    "\n",
    "\n",
    "@eqx.filter_jit\n",
    "def all_grads_eval_pseudo(deltas, env, agent, key, factor=1.0, N=2, episode_length=200):\n",
    "    if N == 1:\n",
    "      reset_keys = jr.split(key, 2)\n",
    "    else:\n",
    "      reset_keys = jr.split(key, N)\n",
    "    eval_first_state = env.reset(reset_keys)\n",
    "    rewards = generate_unroll_mag_pseudo(\n",
    "        key,\n",
    "        env,\n",
    "        eval_first_state,\n",
    "        agent, deltas, factor,\n",
    "        unroll_length=episode_length,\n",
    "        extra_fields=(\"truncation\",),\n",
    "    )\n",
    "    if N == 1:\n",
    "      return jnp.sum(rewards[1].reward[:, 0])\n",
    "    else:\n",
    "      return jnp.sum(rewards[1].reward.mean(axis=1))"
   ],
   "metadata": {
    "id": "1RmP26sRSncd",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:16.037338657Z",
     "start_time": "2023-11-26T14:18:16.027269202Z"
    }
   },
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "import brax"
   ],
   "metadata": {
    "id": "AO3y_VorTXqD",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:16.037690264Z",
     "start_time": "2023-11-26T14:18:16.030882261Z"
    }
   },
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@eqx.filter_jit\n",
    "def actor_step_gradded(key: jr.PRNGKey, env, env_state, policy: Callable, extra_fields):\n",
    "    \"\"\"Makes a single step with the provided policy in the environment.\"\"\"\n",
    "    keys_policy = jr.split(key, env_state.obs.shape[0])\n",
    "    action, _ = eqx.filter_vmap(policy)(keys_policy, env_state.obs)\n",
    "    next_state = env.step(env_state, action.transformed)\n",
    "\n",
    "    return next_state, Transition(\n",
    "        observation=env_state.obs,\n",
    "        action=action,\n",
    "        reward=next_state.reward,\n",
    "        next_observation=next_state.obs,\n",
    "        # extract requested additional fields\n",
    "        extras={x: next_state.info[x] for x in extra_fields},\n",
    "    )\n",
    "\n",
    "def generate_unroll_jac(\n",
    "    key: jr.PRNGKey, env, env_state, agent, unroll_length, extra_fields\n",
    "):\n",
    "\n",
    "    def _len():\n",
    "      return env_state.pipeline_state.x_i.pos.size + env_state.pipeline_state.x_i.rot.size\n",
    "\n",
    "    def _get(s, i):\n",
    "      return jax.lax.cond(\n",
    "          i < env_state.pipeline_state.x_i.pos.size,\n",
    "          lambda: s.pipeline_state.x_i.pos.flatten()[i],\n",
    "          lambda: s.pipeline_state.x_i.rot.flatten()[i - env_state.pipeline_state.x_i.pos.size]\n",
    "      )\n",
    "\n",
    "    def pack(s):\n",
    "        return jnp.concatenate([s.pos.flatten(), s.rot.flatten()])\n",
    "\n",
    "    def unpack(s):\n",
    "        lena = env_state.pipeline_state.x_i.pos.flatten().shape[0]\n",
    "        lenb = env_state.pipeline_state.x_i.rot.flatten().shape[0]\n",
    "        return brax.Transform(s[:lena].reshape(env_state.pipeline_state.x_i.pos.shape), s[lena:].reshape(env_state.pipeline_state.x_i.rot.shape))\n",
    "\n",
    "\n",
    "    def f(carry, cons):\n",
    "        current_key, state = carry\n",
    "        current_key, next_key = jr.split(current_key)\n",
    "\n",
    "        next_state, transition = actor_step_gradded(\n",
    "            current_key, env, state, agent.get_action, extra_fields=extra_fields\n",
    "        )\n",
    "\n",
    "        def compute_jac():\n",
    "            def preproc(g_state, index):\n",
    "                state_i = eqx.tree_at(lambda x: x.pipeline_state.x_i, state, unpack(g_state))\n",
    "                out = _get(actor_step_gradded(\n",
    "                    current_key, env, state_i, agent.get_action, extra_fields=extra_fields\n",
    "                )[0], index)\n",
    "                return out\n",
    "            def p_grad(index, state_i):\n",
    "                return eqx.filter_grad(jtu.Partial(preproc, index=index))(pack(state_i.pipeline_state.x_i))\n",
    "            return eqx.filter_vmap(jtu.Partial(p_grad, state_i=state))(jnp.arange(_len()))\n",
    "        jac = compute_jac()\n",
    "        print(jac.shape)\n",
    "\n",
    "        return (next_key, next_state), jac\n",
    "    (_, final_state), data = filter_scan(f, (key, env_state), None, length=unroll_length)\n",
    "    return final_state, data\n",
    "\n",
    "@eqx.filter_jit\n",
    "def all_grads_eval_jac(env, agent, key, N=2, episode_length=200):\n",
    "    reset_keys = jr.split(key, N)\n",
    "    eval_first_state = env.reset(reset_keys)\n",
    "    final_state, jacs = generate_unroll_jac(\n",
    "        key,\n",
    "        env,\n",
    "        eval_first_state,\n",
    "        agent,\n",
    "        unroll_length=episode_length,\n",
    "        extra_fields=(\"truncation\",),\n",
    "    )\n",
    "    return jacs"
   ],
   "metadata": {
    "id": "rLIf-932JKoR",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:16.079896312Z",
     "start_time": "2023-11-26T14:18:16.039179948Z"
    }
   },
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@eqx.filter_jit\n",
    "def eval_magic(key, **kwargs):\n",
    "  return all_grads_eval_jac(env, agent, key=key, **kwargs)"
   ],
   "metadata": {
    "id": "JGt0id_bpJX1",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:18:16.080231845Z",
     "start_time": "2023-11-26T14:18:16.079741935Z"
    }
   },
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "jax.clear_caches()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-26T14:44:40.687562611Z",
     "start_time": "2023-11-26T14:44:40.189265810Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "jacs = []\n",
    "for i in tqdm(range(32)):\n",
    "  vmapped = eqx.filter_vmap(jtu.Partial(eval_magic, N=1, episode_length=100))\n",
    "  jacs.append(vmapped(jr.split(jr.PRNGKey(i), 8)))\n",
    "jacs = jnp.array(jacs)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "JLU1PdhzLzCo",
    "outputId": "8acf5422-08c9-48e7-9496-ab3ded284391",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:45:59.062366049Z",
     "start_time": "2023-11-26T14:44:41.232197049Z"
    }
   },
   "execution_count": 30,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/32 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 63)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [01:17<00:00,  2.43s/it]\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "jacs = jacs.reshape((-1, 100, 63, 63))\n",
    "jacs = jnp.flip(jacs, axis=1)"
   ],
   "metadata": {
    "id": "gqQVpwn6aUTI",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:47:57.908475741Z",
     "start_time": "2023-11-26T14:47:57.826322401Z"
    }
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "eig = jax.lax.map(lambda x: jax.lax.map(lambda y: jax.numpy.linalg.eigh(y)[0][-1], x), jacs)"
   ],
   "metadata": {
    "id": "R6yfjDc2QcJ2",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:48:23.249734788Z",
     "start_time": "2023-11-26T14:48:06.751765719Z"
    }
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "print(eig.mean())\n",
    "print(jnp.exp(jnp.mean(jnp.log(eig[eig > 0]))))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5OuJM_2JQ6Rh",
    "outputId": "d3c8a4d2-deca-4123-ff7b-e3c156faf430",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.257501306Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for i in range(20):\n",
    "  print(jax.numpy.linalg.eigh(jacs[0, 50 + i])[0][-1])"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 365
    },
    "id": "xRdQAMetrIXN",
    "outputId": "4297d109-2fd6-47ce-bac8-ef4710b03300",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:20:24.259783563Z",
     "start_time": "2023-11-26T14:20:24.259626372Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(jacs.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rybctMWlSFsY",
    "outputId": "69bb83a6-0edd-4d80-a28d-005f3e2e66bb",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:20:24.263504696Z",
     "start_time": "2023-11-26T14:20:24.261783538Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "prd = jnp.eye(63)\n",
    "for i in range(100):\n",
    "  prd = prd @ jacs[1, i]\n",
    "print(jnp.exp(0.01 * jnp.log(jax.numpy.linalg.eigh(prd)[0][-1])))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U2UNDJStw7QM",
    "outputId": "d08aab5e-e1ba-48c0-c510-01289d1e4cc2",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:20:24.272376979Z",
     "start_time": "2023-11-26T14:20:24.263993101Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(jnp.exp(jnp.mean(jax.vmap(lambda x: jnp.log(jax.numpy.linalg.eigh(x)[0][-1]))(jacs[:, 0]))))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "OrmSUl6Uc8W9",
    "outputId": "9938db5a-17e3-4bff-cb97-0698a02d6fff",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.266099992Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@eqx.filter_jit\n",
    "def eval_magic(key, **kwargs):\n",
    "  return eqx.filter_grad(all_grads_eval_pseudo)(jnp.zeros((100,)), env, agent, key=key, **kwargs) #1->1e-9; e+05->e-03\n",
    "\n",
    "raw_grad_t = []\n",
    "for i in tqdm(range(32)):\n",
    "  vmapped = eqx.filter_vmap(jtu.Partial(eval_magic, factor=jnp.array(1), N=1, episode_length=100))\n",
    "  raw_grad_t.append(vmapped(jr.split(jr.PRNGKey(i), 32)))\n",
    "raw_grad_t = jnp.array(raw_grad_t)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zGGWt9IODP8e",
    "outputId": "799d436b-5956-47f8-ad74-d39157529efa",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.268088800Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "raw_grad_t = jnp.flip(jnp.array(raw_grad_t).reshape(-1, raw_grad_t.shape[-1]), axis=1)"
   ],
   "metadata": {
    "id": "lrCybxmx7l9D",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.270495393Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "plt.plot(raw_grad_t.mean(axis=0))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 448
    },
    "id": "kFMKDxmB7StA",
    "outputId": "2ff586ec-9ed9-4eab-a6a8-8fd3e5fe6433",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:20:24.276220742Z",
     "start_time": "2023-11-26T14:20:24.272616871Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "@eqx.filter_jit\n",
    "def evalg4(key, **kwargs):\n",
    "  return eqx.filter_grad(all_grads_eval_pseudo)(jnp.zeros((200,)), env, agent_r, key=key, **kwargs) #1->1e-9; e+05->e-03\n"
   ],
   "metadata": {
    "id": "3Z0ubKlEZdmH",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.274607656Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "l4 = []\n",
    "for i in tqdm(range(32)):\n",
    "  vmapped = eqx.filter_vmap(jtu.Partial(evalg4, factor=jnp.array(1 / 1.05), N=2, episode_length=200))\n",
    "  l4.append(vmapped(jr.split(jr.PRNGKey(i), 32)))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "EQutYa9dYSBZ",
    "outputId": "fd535d05-58a5-417a-be44-64d602470d67",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.275738268Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "l4 = jnp.array(l4)\n",
    "data_ts = jnp.flip(l4.reshape(-1, 200), axis=1)\n",
    "print(jnp.median(data_ts))\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(jnp.median(data_ts, axis=0), 'r', label='median (1k samples)')\n",
    "ax.set_xlabel(\"length of grad eval (singular) (implicit mean of every pair)\")\n",
    "ax.set_ylabel(\"value of ours FOBG for random agent | recalibrated factor\")\n",
    "ax.legend()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "6uzM3GKuY4uS",
    "outputId": "459d74b8-e895-49e2-d83a-2cde5d721c77",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.317681099Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "l3 = jnp.array(l3)\n",
    "data_ts = jnp.flip(l3.reshape(-1, 200), axis=1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(jnp.log(jnp.mean(jnp.abs(data_ts), axis=0)), 'g', label='mean of abs (1k samples)')\n",
    "ax.plot(jnp.log(jnp.median(jnp.abs(data_ts), axis=0)), 'r', label='median of abs (1k samples)')\n",
    "ax.set_xlabel(\"length of grad eval (singular) (implicit mean of every pair)\")\n",
    "ax.set_ylabel(\"log of the absolute value of !ours! FOBG\")\n",
    "ax.legend()"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 949
    },
    "id": "Cz68f55WV_wQ",
    "outputId": "501b0d63-5135-4dc3-febb-2ef34b9e6f02",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.317825944Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "l2 = jnp.array(l2)\n",
    "data_ts = jnp.flip(l2.reshape(-1, 200), axis=1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(jnp.mean(data_ts, axis=0), 'g', label='mean(1k samples)')\n",
    "ax.plot(jnp.median(data_ts, axis=0), 'r', label='median (1k samples)')\n",
    "ax.set_xlabel(\"length of grad eval (singular) (implicit mean of every pair)\")\n",
    "ax.set_ylabel(\"value of standard FOBG\")\n",
    "ax.legend()\n",
    "\n",
    "assert False"
   ],
   "metadata": {
    "id": "Cxv3E22KWpnd",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.317885142Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "l3 = jnp.array(l3)\n",
    "data_ts = jnp.flip(l3.reshape(-1, 200), axis=1)\n",
    "N = data_ts.shape[0]\n",
    "\n",
    "med = jnp.median(data_ts, axis=0)\n",
    "std = 3 * jnp.std(data_ts, axis=0) / jnp.sqrt(N)\n",
    "mean = jnp.mean(data_ts, axis=0)\n",
    "mad = 1.5 * 3 * jnp.median(jnp.absolute(data_ts - med)) / jnp.sqrt(N)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(jnp.mean(data_ts, axis=0), 'g', label='mean(1k samples)')\n",
    "ax.fill_between(jnp.arange(data_ts.shape[1]), mean + std, mean - std, alpha=0.2, color='g')\n",
    "ax.plot(jnp.median(data_ts, axis=0), 'r', label='median (1k samples)')\n",
    "ax.fill_between(jnp.arange(data_ts.shape[1]), med + mad, med-mad, alpha=0.2, color='r')\n",
    "ax.set_xlabel(\"length of grad eval (singular) (implicit mean of every pair)\")\n",
    "ax.set_ylabel(\"value of ours FOBG\")\n",
    "ax.legend()"
   ],
   "metadata": {
    "id": "isViFa-PanEx",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.317937322Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evalg2(key, **kwargs):\n",
    "  return eqx.filter_grad(all_grads_eval)(jnp.zeros((200,)), env, agent, key, **kwargs)\n",
    "l2 = []\n",
    "for i in tqdm(range(32)):\n",
    "  l2.append(eqx.filter_vmap(jtu.Partial(evalg2, episode_length=200, N=2))(jr.split(jr.PRNGKey(i), 32)))"
   ],
   "metadata": {
    "id": "bdPeEPPlAqs6",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.317985052Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "l2 = jnp.array(l2)\n",
    "data_ts = jnp.flip(l2.reshape(-1, 200), axis=1)\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(jnp.log(jnp.mean(jnp.abs(data_ts), axis=0)), 'g', label='mean (1k samples)')\n",
    "ax.plot(jnp.log(jnp.median(jnp.abs(data_ts), axis=0)), 'r', label='median (1k samples)')\n",
    "ax.set_xlabel(\"length of grad eval (singular) (implicit mean of every pair)\")\n",
    "ax.set_ylabel(\"log of the absolute value of the FOBG\")\n",
    "ax.legend()"
   ],
   "metadata": {
    "id": "IfHRVl4OD-g4",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.318034218Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def evalg(key, **kwargs):\n",
    "  return eqx.filter_grad(eval)(jnp.zeros(()), env, agent, key, **kwargs)\n",
    "#evalg = eqx.filter_grad(eval)\n",
    "#print(evalg(jr.PRNGKey(0), jnp.zeros(()), env, agent, episode_length=1, N=1))\n",
    "out = eqx.filter_grad(eval)(jnp.zeros(()), env, agent, jr.PRNGKey(0), episode_length=10, N=2)\n",
    "print(out)"
   ],
   "metadata": {
    "id": "wOwwk_9xp9k2",
    "ExecuteTime": {
     "end_time": "2023-11-26T14:20:24.329651195Z",
     "start_time": "2023-11-26T14:20:24.318081014Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(evalg(jr.PRNGKey(0), episode_length=10, N=2))"
   ],
   "metadata": {
    "id": "tDaAPj__rOxZ",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.318132879Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "vevalg = eqx.filter_vmap(jtu.Partial(evalg, episode_length=200, N=2))(jr.split(jr.PRNGKey(0), 32))\n",
    "\n",
    "l = []\n",
    "for i in tqdm(range(256)):\n",
    "  l.append(eqx.filter_vmap(jtu.Partial(evalg, episode_length=200, N=2))(jr.split(jr.PRNGKey(i), 32)))"
   ],
   "metadata": {
    "id": "tS1FPEoVrUzJ",
    "ExecuteTime": {
     "start_time": "2023-11-26T14:20:24.318180228Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "data = jnp.concatenate(l).flatten()\n",
    "print(data)\n",
    "print(len(data))"
   ],
   "metadata": {
    "id": "nP5PebOrvHeJ"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(jnp.mean(data))\n",
    "print(jnp.median(data))\n",
    "print(jnp.mean(jnp.sort(data)[5:-5]))"
   ],
   "metadata": {
    "id": "XxS4QZMJsVH3"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "pdf = jax.scipy.stats.gaussian_kde(data[len(data)//3:-len(data)//3]).pdf"
   ],
   "metadata": {
    "id": "gvPAAJWP8nqg"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def estimate_maxima(data):\n",
    "    kde = jax.scipy.stats.gaussian_kde(data)\n",
    "    no_samples = 100000\n",
    "    samples = jnp.linspace(jnp.min(data), jnp.max(data), no_samples)\n",
    "    probs = kde.evaluate(samples)\n",
    "    maxima_index = probs.argmax()\n",
    "    maxima = samples[maxima_index]\n",
    "\n",
    "    return maxima\n",
    "deltalen = int(len(data) * 0.44)\n",
    "estimate_maxima(data[deltalen:-deltalen])"
   ],
   "metadata": {
    "id": "xeljPRNh96Jh"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(pdf(jnp.median(data)))\n",
    "print(pdf(0))"
   ],
   "metadata": {
    "id": "jFHJ9jHE864J"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "deltalen = int(len(data) * 0.4)\n",
    "print(jnp.mean(jnp.sort(data)[deltalen:-deltalen]))"
   ],
   "metadata": {
    "id": "8CoQXPIHzoJF"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "hist, bins, _ = plt.hist(data, log=True, bins=len(data) // 10)\n",
    "plt.show()\n",
    "hist, bins, _ = plt.hist(data, bins=len(data) // 10)\n",
    "plt.show()"
   ],
   "metadata": {
    "id": "s9Dz4PTEufWx"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "al = []\n",
    "bl = []\n",
    "\n",
    "from tqdm import tqdm as tqdm\n",
    "\n",
    "for i in tqdm(range(100)):\n",
    "  k = jr.PRNGKey(100000 + i)\n",
    "  a = eval(jnp.zeros(()), env, agent, k, episode_length=200, N=1000)\n",
    "  b = eval(jnp.ones(()) * 1e-3, env, agent, k, episode_length=200, N=1000)\n",
    "  al.append(a)\n",
    "  bl.append(b)"
   ],
   "metadata": {
    "id": "uapuFRUb406o"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "a = jnp.array(al[len(al)//2:]).mean()\n",
    "b = jnp.array(bl[len(al)//2:]).mean()"
   ],
   "metadata": {
    "id": "_2z9YzKL52_k"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print((a-b)/(1e-3))"
   ],
   "metadata": {
    "id": "TjoFbJkS5LwD"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#out = eqx.filter_grad(eval)(jnp.zeros(()), env, agent, jr.PRNGKey(0), episode_length=10)\n",
    "#print(out) # 1.5"
   ],
   "metadata": {
    "id": "EVAGt2h1PR2j"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#out = eqx.filter_grad(eval)(jnp.zeros(()), env, agent, jr.PRNGKey(0), episode_length=40)\n",
    "#print(out) # 12"
   ],
   "metadata": {
    "id": "1WvEa7g9PNCM"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hH5ac5WLp07O"
   },
   "outputs": [],
   "source": [
    "#outs = []\n",
    "#for i in range(50):\n",
    "#  outs.append(eqx.filter_grad(eval)(jnp.zeros(()), env, agent, jr.PRNGKey(i), episode_length=40))\n",
    "#outs = jnp.array(outs)\n",
    "#print(jnp.mean(outs), jnp.std(outs))"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "#outs = []\n",
    "#for i in range(50):\n",
    "#  outs.append(eqx.filter_grad(eval)(jnp.zeros(()), env, agent, jr.PRNGKey(i), episode_length=10))\n",
    "#outs = jnp.array(outs)\n",
    "#print(jnp.mean(outs), jnp.std(outs))"
   ],
   "metadata": {
    "id": "UP01_Zxriy_n"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "x = jtu.Partial(eval, delta=jnp.zeros(()), env=env, agent=agent, episode_length=400)(jr.PRNGKey(0))"
   ],
   "metadata": {
    "id": "SpG-ErPMg_gi"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "print(x)"
   ],
   "metadata": {
    "id": "az89BUQlhGNW"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "outs = eqx.filter_vmap(jtu.Partial(evalg, delta=jnp.zeros(()), env=env, agent=agent, episode_length=400))(jr.split(jr.PRNGKey(0), 32))\n",
    "outs = jnp.array(outs)\n",
    "print(jnp.mean(outs), jnp.std(outs))"
   ],
   "metadata": {
    "id": "P9OC-_o_i7fA"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install tqdm\n",
    "from tqdm import tqdm as tqdm\n",
    "outs_r = []\n",
    "for i in tqdm(range(5000)):\n",
    "  outs_r.append(eqx.filter_grad(eval)(jnp.zeros(()), env, agent_r, jr.PRNGKey(i), episode_length=400))\n",
    "outs_r = jnp.array(outs_r)\n",
    "print(jnp.mean(outs_r), jnp.std(outs_r))"
   ],
   "metadata": {
    "id": "rUqViAUQeoV9"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "#outs = []\n",
    "#for i in range(50):\n",
    "#  outs.append(eqx.filter_grad(eval)(jnp.zeros(()), env, agent_r, jr.PRNGKey(i), episode_length=10))\n",
    "#outs = jnp.array(outs)\n",
    "#print(jnp.mean(outs), jnp.std(outs))"
   ],
   "metadata": {
    "id": "LYLuCVl7nROB"
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 10 -> 1.5\n",
    "# 40 -> 12\n",
    "# 160 -> 12000\n",
    "# 640 -> 6 * 10e18"
   ],
   "metadata": {
    "id": "jXOS3bDGNHa0"
   },
   "execution_count": null,
   "outputs": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "machine_shape": "hm",
   "gpuType": "V100"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
