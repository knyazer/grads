{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2993a3a7bfc8587f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:54.425921936Z",
     "start_time": "2024-02-22T19:34:53.743171846Z"
    }
   },
   "outputs": [],
   "source": [
    "import jax \n",
    "import jax.numpy as jnp, jax.random as jr, jax.tree_util as jtu\n",
    "import equinox as eqx\n",
    "import equinox.internal as eqxi\n",
    "from typing import Callable\n",
    "import time\n",
    "import logging\n",
    "\n",
    "class LogNormalDistribution(eqx.Module):\n",
    "    \"\"\"Multivariate Log Normal distribution with diagonal covariance\"\"\"\n",
    "\n",
    "    mean: jax.Array\n",
    "    log_std: jax.Array\n",
    "    \n",
    "    def __init__(self, mean: jax.Array, log_std: jax.Array):\n",
    "        self.mean = mean\n",
    "        self.log_std = log_std\n",
    "\n",
    "    def get_pdf(self, value):\n",
    "        value = eqx.error_if(\n",
    "            value,\n",
    "            value.shape != self.mean.shape,\n",
    "            \"Wrong shapes for the mean/value of action distr\",\n",
    "        )\n",
    "        value = eqx.error_if(\n",
    "            value,\n",
    "            value.shape != self.log_std.shape,\n",
    "            \"Wrong shapes for the std/value of action distr\",\n",
    "        )\n",
    "\n",
    "        normalized = (value - self.mean) / jnp.exp(self.log_std)\n",
    "        return jax.scipy.stats.norm.logpdf(normalized).sum()\n",
    "\n",
    "    def sample(self, key: jr.PRNGKey):\n",
    "        return jr.normal(key, self.mean.shape) * jnp.exp(self.log_std) + self.mean\n",
    "\n",
    "    def entropy(self):\n",
    "        return self.log_std.sum() * 0.5  # entropy without constant factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9d4bbc2e-09a0-420e-bbb6-0063449c9f36",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.721359952Z",
     "start_time": "2024-02-22T19:34:54.465301025Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Feb 22 20:34:55 2024       \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 545.29.06              Driver Version: 545.29.06    CUDA Version: 12.3     |\r\n",
      "|-----------------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|                                         |                      |               MIG M. |\r\n",
      "|=========================================+======================+======================|\r\n",
      "|   0  NVIDIA GeForce RTX 3050 ...    Off | 00000000:01:00.0 Off |                  N/A |\r\n",
      "| N/A   44C    P0             752W /  40W |      4MiB /  4096MiB |      0%      Default |\r\n",
      "|                                         |                      |                  N/A |\r\n",
      "+-----------------------------------------+----------------------+----------------------+\r\n",
      "                                                                                         \r\n",
      "+---------------------------------------------------------------------------------------+\r\n",
      "| Processes:                                                                            |\r\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\r\n",
      "|        ID   ID                                                             Usage      |\r\n",
      "|=======================================================================================|\r\n",
      "|    0   N/A  N/A      2238      G   /usr/bin/gnome-shell                          1MiB |\r\n",
      "+---------------------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5f90382960f1e9b",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.730058339Z",
     "start_time": "2024-02-22T19:34:55.727924990Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluates agent on the environment.\n",
    "    It is not jittable, since run_evaluation needs time.time()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, agent, num_eval_envs, episode_length):\n",
    "        self._eval_walltime = 0.0\n",
    "        self.eval_env = eval_env\n",
    "        self.episode_length = episode_length\n",
    "        self.num_eval_envs = num_eval_envs\n",
    "        self._steps_per_unroll = episode_length * num_eval_envs\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def evaluate(self, key: jr.PRNGKey, agent):\n",
    "       def actor_step(key: jr.PRNGKey, env_state, policy: Callable, extra_fields):\n",
    "           \"\"\"Makes a single step with the provided policy in the environment.\"\"\"\n",
    "           keys_policy = jr.split(key, env_state.obs.shape[0])\n",
    "           action = eqx.filter_vmap(policy)(env_state.obs, keys_policy)\n",
    "           next_state = self.eval_env.step(env_state, action.transformed)\n",
    "\n",
    "           return next_state, Transition(\n",
    "               observation=env_state.obs,\n",
    "               action=action,\n",
    "               reward=next_state.reward,\n",
    "               next_observation=next_state.obs,\n",
    "               # extract requested additional fields\n",
    "               extras={x: next_state.info[x] for x in extra_fields},\n",
    "           )\n",
    "\n",
    "       def generate_unroll(\n",
    "           key: jr.PRNGKey, env_state, policy: Callable, unroll_length, extra_fields\n",
    "       ):\n",
    "           \"\"\"Collects trajectories of given unroll length.\"\"\"\n",
    "\n",
    "           def f(carry, _):\n",
    "               current_key, state = carry\n",
    "               current_key, next_key = jr.split(current_key)\n",
    "\n",
    "               next_state, transition = actor_step(\n",
    "                   current_key, state, policy, extra_fields=extra_fields\n",
    "               )\n",
    "               return (next_key, next_state), transition\n",
    "\n",
    "           (_, final_state), data = eqxi.scan(f, (key, env_state), (), length=unroll_length, kind='lax')\n",
    "           return final_state, data \n",
    "       \n",
    "       reset_keys = jr.split(key, self.num_eval_envs)\n",
    "       eval_first_state = self.eval_env.reset(reset_keys)\n",
    "       return generate_unroll(\n",
    "           key,\n",
    "           eval_first_state,\n",
    "           agent,\n",
    "           unroll_length=self.episode_length,\n",
    "           extra_fields=(\"truncation\",),\n",
    "       )[0]\n",
    "\n",
    "    def run_evaluation(\n",
    "        self, key: jr.PRNGKey, agent, training_metrics, aggregate_episodes: bool = True\n",
    "    ):\n",
    "        t = time.time()\n",
    "        eval_state = self.evaluate(key, agent)\n",
    "        eval_metrics = eval_state.info[\"eval_metrics\"]\n",
    "        eval_metrics.active_episodes.block_until_ready()\n",
    "        epoch_eval_time = time.time() - t\n",
    "        metrics = {}\n",
    "        for fn in [jnp.mean, jnp.std]:\n",
    "            suffix = \"_std\" if fn == jnp.std else \"\"\n",
    "            metrics.update(\n",
    "                {\n",
    "                    f\"eval/episode_{name}{suffix}\": (\n",
    "                        fn(value) if aggregate_episodes else value\n",
    "                    )\n",
    "                    for name, value in eval_metrics.episode_metrics.items()\n",
    "                }\n",
    "            )\n",
    "        metrics[\"eval/avg_episode_length\"] = jnp.mean(eval_metrics.episode_steps)\n",
    "        metrics[\"eval/epoch_eval_time\"] = epoch_eval_time\n",
    "        metrics[\"eval/sps\"] = self._steps_per_unroll / epoch_eval_time\n",
    "        self._eval_walltime = self._eval_walltime + epoch_eval_time\n",
    "        metrics = {\"eval/walltime\": self._eval_walltime, **training_metrics, **metrics}\n",
    "\n",
    "        return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "initial_id",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.746201956Z",
     "start_time": "2024-02-22T19:34:55.732295018Z"
    }
   },
   "outputs": [],
   "source": [
    "class Action(eqx.Module):\n",
    "    \"\"\"\n",
    "    Action class represents a single action taken by an agent.\n",
    "    Additionally stores some useful data.\n",
    "\n",
    "    raw: action that was a direct output of an Actor-Critic model\n",
    "    transformed: action that was applied on the environment\n",
    "    distr: distribution from which raw action was sampled\n",
    "    \"\"\"\n",
    "\n",
    "    raw: jax.Array = None\n",
    "    transformed: jax.Array = None\n",
    "    distr: LogNormalDistribution = None\n",
    "    \n",
    "    def __init__(self, raw, transformed, distr):\n",
    "        self.raw = raw\n",
    "        self.transformed = transformed\n",
    "        self.distr = distr\n",
    "\n",
    "    def postprocess(self, apply: Callable):\n",
    "        return Action(\n",
    "            raw=self.raw, transformed=apply(self.transformed), distr=self.distr\n",
    "        )\n",
    "\n",
    "\n",
    "class ValueRange(eqx.Module):\n",
    "    low: jax.Array\n",
    "    high: jax.Array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "60e1d49a2a305d8a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.750369721Z",
     "start_time": "2024-02-22T19:34:55.743291651Z"
    }
   },
   "outputs": [],
   "source": [
    "class MeanNetwork(eqx.Module):\n",
    "    structure: list\n",
    "\n",
    "    def __init__(self, key: jr.PRNGKey, observation_size: int, action_size: int):\n",
    "        key1, key2, key3, key4 = jax.random.split(key, 4)\n",
    "        self.structure = [\n",
    "            eqx.nn.Linear(observation_size, 64, key=key1),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, 64, key=key2),\n",
    "            jax.nn.tanh,\n",
    "            eqx.nn.Linear(64, action_size, key=key4),\n",
    "        ]\n",
    "\n",
    "        # scaling down the weights of the output layer improves performance\n",
    "        self.structure = eqx.tree_at(\n",
    "            where=lambda s: s[-1].weight,\n",
    "            pytree=self.structure,\n",
    "            replace_fn=lambda weight: weight * jnp.array(0.1),\n",
    "        )\n",
    "\n",
    "    def __call__(self, x):\n",
    "        for operator in self.structure:\n",
    "            x = operator(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "206a99b19d28971a",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.878838726Z",
     "start_time": "2024-02-22T19:34:55.751787339Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA backend failed to initialize: Found CUDA version 12010, but JAX was built against version 12020, which is newer. The copy of CUDA that is installed must be at least as new as the version against which JAX was built. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "class RunningMeanStd(eqx.Module):\n",
    "    mean: jax.Array\n",
    "    M2: jax.Array  # sum of second moments of the samples (sum of variances)\n",
    "    n: jax.Array\n",
    "    size: int = eqx.field(static=True)\n",
    "\n",
    "    # we are initializing n with two so that we don't get division by zero, ever\n",
    "    # this biases the running statistics, but not really that much\n",
    "    def __init__(self, size, mean=None, M2=None, n=jnp.int32(2)):\n",
    "        self.size = size\n",
    "        self.mean = (jnp.zeros(size)) if mean is None else mean\n",
    "        self.M2 = (jnp.zeros(size) + 1e-6) if M2 is None else M2\n",
    "        self.n = n\n",
    "\n",
    "    def __call__(self, obs, eval=False):\n",
    "        std = jnp.sqrt(self.M2 / self.n)\n",
    "        if not eval:\n",
    "            std = eqx.error_if(\n",
    "                std,\n",
    "                std.shape != obs.shape,\n",
    "                \"Standard deviation should have the same shape as the observation, \"\n",
    "                + f\"std shape is {std.shape} but observation shape is {obs.shape}\",\n",
    "            )\n",
    "            # error if std is complex or nan\n",
    "            std = eqx.error_if(\n",
    "                std,\n",
    "                jnp.any(jnp.isnan(std)) | jnp.any(jnp.iscomplex(std)),\n",
    "                \"Standard deviation should not be nan or complex\",\n",
    "            )\n",
    "\n",
    "        # clip std, so that we don't get extreme values\n",
    "        std = jnp.clip(std, 1e-6, 1e6)\n",
    "\n",
    "        # clip the extreme outliers -> more stability during training.\n",
    "        # by Chebyshev inequality, ~99% of values are not clipped.\n",
    "        processed = jnp.clip((obs - jax.lax.stop_gradient(self.mean)) / jax.lax.stop_gradient(std), -10, 10)\n",
    "\n",
    "        return processed\n",
    "\n",
    "    def update_single(self, obs):\n",
    "        return self.update_batched(obs[None, :])\n",
    "\n",
    "    def update_batched(self, obs):\n",
    "        obs = eqx.error_if(\n",
    "            obs,\n",
    "            len(obs.shape) != 2 or obs.shape[1] != self.size,\n",
    "            f\"Batched observation should have the shape of (_, {self.size}),\"\n",
    "            + f\"but got {obs.shape}\",\n",
    "        )\n",
    "\n",
    "        n = self.n + obs.shape[0]\n",
    "\n",
    "        diff_to_old_mean = obs - self.mean\n",
    "        new_mean = self.mean + diff_to_old_mean.sum(axis=0) / n\n",
    "\n",
    "        diff_to_new_mean = obs - new_mean\n",
    "        var_upd = jnp.sum(diff_to_old_mean * diff_to_new_mean, axis=0)\n",
    "        M2 = self.M2 + var_upd\n",
    "\n",
    "        return RunningMeanStd(self.size, mean=new_mean, M2=M2, n=n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1ea3835e3876bf4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.880426552Z",
     "start_time": "2024-02-22T19:34:55.861603949Z"
    }
   },
   "outputs": [],
   "source": [
    "class Actor(eqx.Module):\n",
    "    \"\"\"A module, that outputs action distribution for a particular state.\"\"\"\n",
    "\n",
    "    mean_network: MeanNetwork\n",
    "    log_std: jax.Array  # Trainable array\n",
    "    normalizer: RunningMeanStd\n",
    "    constraint: Callable\n",
    "\n",
    "    def __init__(\n",
    "            self,\n",
    "            key: jr.PRNGKey,\n",
    "            observation_size: int,\n",
    "            action_size: int,\n",
    "            initial_std: float = 0.5,\n",
    "    ):\n",
    "        self.mean_network = MeanNetwork(key, observation_size, action_size)\n",
    "        self.log_std = jnp.ones((action_size,)) * jnp.log(initial_std)\n",
    "        self.normalizer = RunningMeanStd(observation_size)\n",
    "        # tanh constraint is applied by default\n",
    "        self.constraint = lambda x: jnp.tanh(x)\n",
    "\n",
    "    def __call__(self, x, key=None, eval=False):\n",
    "        x = self.normalizer(x, eval=eval)\n",
    "        distr = LogNormalDistribution(self.mean_network(x), self.log_std)\n",
    "        action = distr.sample(key)\n",
    "        action = self.constraint(action)\n",
    "        return Action(raw=x, transformed=action, distr=distr) \n",
    "    \n",
    "    def get_trainable(self):\n",
    "        \"\"\"Returns the PyTree of trainable parameters.\"\"\"\n",
    "        return eqx.filter(self, eqx.is_inexact_array)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4defc1879735a0d1",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:55.909207969Z",
     "start_time": "2024-02-22T19:34:55.866591064Z"
    }
   },
   "outputs": [],
   "source": [
    "class Transition(eqx.Module):\n",
    "    \"\"\"Represents a transition between two adjacent environment states.\"\"\"\n",
    "\n",
    "    observation: jax.Array  # observation on the current state\n",
    "    action: Action  # action that was taken on the current state\n",
    "    reward: float  # reward, that was given as the result of the action\n",
    "    next_observation: jax.Array  # next observation\n",
    "    extras: dict  # any simulator-extracted hints, like end of the episode signal\n",
    "    \n",
    "    def __init__(self, observation, action, reward, next_observation, extras = {}):\n",
    "        self.observation = observation\n",
    "        self.action = action\n",
    "        self.reward = reward\n",
    "        self.next_observation = next_observation\n",
    "        self.extras = extras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "de49d8dc04bffc86",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:34:57.583472261Z",
     "start_time": "2024-02-22T19:34:55.907402781Z"
    }
   },
   "outputs": [],
   "source": [
    "from brax import envs\n",
    "import optax\n",
    "import jax.debug as jdb\n",
    "import pdb\n",
    "from brax.envs import training\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d0898d5f87e31deb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:35:02.954974467Z",
     "start_time": "2024-02-22T19:34:57.620072021Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[34m\u001B[1mwandb\u001B[0m: Currently logged in as: \u001B[33mknyaz\u001B[0m. Use \u001B[1m`wandb login --relogin`\u001B[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/home/knyaz/workspace/grads/wandb/run-20240222_203502-6l7zd90p</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/knyaz/grads/runs/6l7zd90p' target=\"_blank\">glowing-mandu-77</a></strong> to <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">https://wandb.ai/knyaz/grads</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/knyaz/grads/runs/6l7zd90p' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/6l7zd90p</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/knyaz/grads/runs/6l7zd90p?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>",
      "text/plain": "<wandb.sdk.wandb_run.Run at 0x7fe29553a200>"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "jax.config.update('jax_log_compiles', True)\n",
    "wandb.login()\n",
    "wandb.init(\n",
    "    # Set the project where this run will be logged\n",
    "    project=\"grads\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba0008ddf4d4fda4",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-22T19:35:02.988926203Z",
     "start_time": "2024-02-22T19:35:02.963216008Z"
    }
   },
   "outputs": [],
   "source": [
    "def lyapunov_schedule(lyapunov_factor, epoch_index):\n",
    "    return lyapunov_factor\n",
    "\n",
    "def env_step(\n",
    "        carry,\n",
    "        step_index: int,\n",
    "        epoch_index: int,\n",
    "        zero_grad_agent = None,\n",
    "        gradded_zero_agent = None\n",
    "):\n",
    "    env_state, key = carry\n",
    "    key, key_sample = jax.random.split(key)\n",
    "\n",
    "    dyn_zero_grad, stat = eqx.partition(zero_grad_agent, eqx.is_array)\n",
    "    dyn_gradded_zero, _ = eqx.partition(gradded_zero_agent, eqx.is_array)\n",
    "\n",
    "    # truncate indicator\n",
    "    mod = jnp.mod(step_index + 1, truncation_length)\n",
    "    lyapunov_multiplier = lyapunov_schedule(lyapunov_factor, epoch_index)\n",
    "    reward_multiplier = time_discount ** (mod - truncation_length)\n",
    "\n",
    "    dyn_combined = jtu.tree_map(lambda nograd, grad: nograd + grad, dyn_zero_grad, dyn_gradded_zero)\n",
    "    restored_agent = eqx.combine(dyn_combined, stat)\n",
    "\n",
    "    actions = eqx.filter_vmap(restored_agent)(env_state.obs, jax.random.split(key_sample, num_envs))\n",
    "    next_state = env.step(env_state, actions.transformed)\n",
    "    # stop the gradient for the next state, mul by luapunov schedule\n",
    "    next_state_grad = jtu.tree_map(lambda x: x - jax.lax.stop_gradient(x), next_state)\n",
    "    next_state = jtu.tree_map(lambda no_grad, grad: no_grad + grad * lyapunov_multiplier, jax.lax.stop_gradient(next_state), next_state_grad)\n",
    "    \n",
    "    if truncation_length is not None:\n",
    "        next_state = jax.lax.cond(\n",
    "            mod == 0.,\n",
    "            jax.lax.stop_gradient, lambda x: x, next_state)\n",
    "\n",
    "    return (next_state, key), (next_state.reward * reward_multiplier, env_state.obs)\n",
    "\n",
    "def loss(agent, key=None, epoch_index=None):\n",
    "    assert epoch_index is not None\n",
    "    assert key is not None\n",
    "    dyn, stat = eqx.partition(agent, eqx.is_array)\n",
    "    dyn_stopped = jtu.tree_map(lambda x: jax.lax.stop_gradient(x), dyn)\n",
    "    zero_grad_agent = eqx.combine(dyn_stopped, stat)\n",
    "    zero_dyn = jtu.tree_map(lambda d, d_stopped: d - d_stopped, dyn, dyn_stopped)\n",
    "    zero_dyn = eqx.error_if(zero_dyn, ~jtu.tree_reduce(lambda acc, x: acc & jnp.all(x == 0.0), zero_dyn, True), \"zero_grad_agent is not zero\")\n",
    "    gradded_zero_agent = eqx.combine(zero_dyn, stat)\n",
    "\n",
    "    key_reset, key_scan = jax.random.split(key)\n",
    "    env_state = env.reset(jax.random.split(key_reset, num_envs))\n",
    "\n",
    "    _, (rewards, obs) = jax.lax.scan(\n",
    "        jax.tree_util.Partial(env_step, epoch_index=epoch_index, zero_grad_agent=zero_grad_agent, gradded_zero_agent=gradded_zero_agent),\n",
    "        init=(env_state, key_scan),\n",
    "        xs=jnp.arange(episode_length),\n",
    "        length=episode_length\n",
    "    )\n",
    "\n",
    "    assert rewards.shape == (episode_length, num_envs)\n",
    "\n",
    "    # loss is (inverted) sum of the mean rewards\n",
    "    return -jnp.sum(jnp.mean(rewards, axis=1)), obs\n",
    "loss_grad = eqx.filter_value_and_grad(loss, has_aux=True)\n",
    "\n",
    "@eqx.filter_jit\n",
    "def training_epoch(agent, opt_state, key=None, optimizer=None, epoch_index=None):\n",
    "    assert key is not None\n",
    "    assert optimizer is not None\n",
    "    assert epoch_index is not None\n",
    "    key, key_grad = jax.random.split(key)\n",
    "    (value, obs), grad = loss_grad(agent, key=key_grad, epoch_index=epoch_index)\n",
    "    params_update, new_opt_state = optimizer.update(grad, opt_state, agent)\n",
    "    # check the learning rate\n",
    "    new_agent = eqx.apply_updates(agent, params_update)\n",
    "    # update normalizer\n",
    "    new_normalizer = new_agent.normalizer.update_batched(\n",
    "        obs.reshape((-1, env.observation_size))\n",
    "    )\n",
    "    new_agent = eqx.tree_at(\n",
    "        where=lambda s: s.normalizer,\n",
    "        pytree=new_agent,\n",
    "        replace_fn=lambda _: new_normalizer,\n",
    "    )\n",
    "    return new_agent, new_opt_state, obs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e57fc6a81297971a",
   "metadata": {
    "collapsed": false,
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:48.007119542Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:e7lu3z7w) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.107 MB of 0.107 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "04b301adff2b49ffa6acff5c2e152198"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B sync reduced upload amount by 2.5%             "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_log_std</td><td>███▇▇▇▆▆▆▆▆▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/episode_reward</td><td>▁▁▂▂▂▃▃▃▃▃▄▄▄▅▅▅▆▆▆▆▇▆▆▆▇▇▇▇▇█▇█▇██▇▇▇██</td></tr><tr><td>lyapunov_factor</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_log_std</td><td>-2.62198</td></tr><tr><td>eval/episode_reward</td><td>1657.04773</td></tr><tr><td>lyapunov_factor</td><td>0.93</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">feb21-crt*:lr=(0.003, 0.0005),tr=100,lf=0.93,td=0.98</strong> at: <a href='https://wandb.ai/knyaz/grads/runs/e7lu3z7w' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/e7lu3z7w</a><br/> View job at <a href='https://wandb.ai/knyaz/grads/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MTYxNTU1MA==/version_details/v18' target=\"_blank\">https://wandb.ai/knyaz/grads/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MTYxNTU1MA==/version_details/v18</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240222_203503-e7lu3z7w/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:e7lu3z7w). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112154755068736, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "974320a20db642008b367dd8c422e65c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/home/knyaz/workspace/grads/wandb/run-20240222_232948-um6t01sj</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/knyaz/grads/runs/um6t01sj' target=\"_blank\">feb21-crt*:lr=(0.003, 0.001),tr=100,lf=0.88,td=0.98</a></strong> to <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">https://wandb.ai/knyaz/grads</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/knyaz/grads/runs/um6t01sj' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/um6t01sj</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished tracing + transforming training_epoch for pjit in 8.464920282363892 sec\n",
      "Compiling training_epoch for with global shapes and types [ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[]), ShapedArray(uint32[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "Finished jaxpr to MLIR module conversion jit(training_epoch) in 6.9565348625183105 sec\n",
      "Finished XLA compilation of jit(training_epoch) in 53.836385011672974 sec\n",
      "Finished tracing + transforming evaluate for pjit in 1.3159773349761963 sec\n",
      "Compiling evaluate for with global shapes and types [ShapedArray(uint32[2]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "Finished jaxpr to MLIR module conversion jit(evaluate) in 0.732659101486206 sec\n",
      "Finished XLA compilation of jit(evaluate) in 5.047026872634888 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 128.86610412597656\n",
      "reward 170.19944763183594\n",
      "reward 213.5319366455078\n",
      "reward 241.5941162109375\n",
      "reward 289.4486999511719\n",
      "reward 269.00732421875\n",
      "reward 311.1708068847656\n",
      "reward 341.0157775878906\n",
      "reward 393.63720703125\n",
      "reward 460.30255126953125\n",
      "reward 542.6239013671875\n",
      "reward 515.9552612304688\n",
      "reward 557.1414184570312\n",
      "reward 654.4070434570312\n",
      "reward 707.3300170898438\n",
      "reward 719.4854736328125\n",
      "reward 785.4581298828125\n",
      "reward 729.981201171875\n",
      "reward 846.1466674804688\n",
      "reward 871.2161254882812\n",
      "reward 932.7716674804688\n",
      "reward 881.4666137695312\n",
      "reward 962.56982421875\n",
      "reward 1003.6636962890625\n",
      "reward 998.0379028320312\n",
      "reward 984.2429809570312\n",
      "reward 1048.552001953125\n",
      "reward 1148.4229736328125\n",
      "reward 1129.2254638671875\n",
      "reward 1174.764404296875\n",
      "reward 1177.3607177734375\n",
      "reward 1242.18994140625\n",
      "reward 1255.952880859375\n",
      "reward 1305.3746337890625\n",
      "reward 1315.8084716796875\n",
      "reward 1352.4869384765625\n",
      "reward 1366.36669921875\n",
      "reward 1447.623291015625\n",
      "reward 1408.50146484375\n",
      "reward 1303.8580322265625\n",
      "reward 1394.973388671875\n",
      "reward 1476.31591796875\n",
      "reward 1413.1859130859375\n",
      "reward 1556.0765380859375\n",
      "reward 1467.4619140625\n",
      "reward 1487.4088134765625\n",
      "reward 1492.139404296875\n",
      "reward 1503.8280029296875\n",
      "reward 1567.4383544921875\n",
      "reward 1599.830322265625\n",
      "reward 1520.2841796875\n",
      "reward 1522.34814453125\n",
      "reward 1612.4840087890625\n",
      "reward 1516.5194091796875\n",
      "reward 1544.6322021484375\n",
      "reward 1572.8807373046875\n",
      "reward 1654.2242431640625\n",
      "reward 1669.3245849609375\n",
      "reward 1686.7164306640625\n",
      "reward 1703.942626953125\n",
      "reward 1624.1781005859375\n",
      "reward 1668.5081787109375\n",
      "reward 1675.9163818359375\n",
      "reward 1751.294189453125\n",
      "reward 1718.447509765625\n",
      "reward 1722.7640380859375\n",
      "reward 1684.72265625\n",
      "reward 1706.14453125\n",
      "reward 1688.7645263671875\n",
      "reward 1709.341064453125\n",
      "reward 1765.496337890625\n",
      "reward 1758.0965576171875\n",
      "reward 1771.659423828125\n",
      "reward 1835.7191162109375\n",
      "reward 1729.0107421875\n",
      "reward 1763.1927490234375\n",
      "reward 1792.2711181640625\n",
      "reward 1839.938720703125\n",
      "reward 1810.6942138671875\n",
      "reward 1834.6063232421875\n",
      "reward 1905.8250732421875\n",
      "reward 1857.46484375\n",
      "reward 1839.637939453125\n",
      "reward 1846.8172607421875\n",
      "reward 1875.7581787109375\n",
      "reward 1926.919189453125\n",
      "reward 1946.630126953125\n",
      "reward 1891.245361328125\n",
      "reward 1862.9276123046875\n",
      "reward 1945.2047119140625\n",
      "reward 1961.9664306640625\n",
      "reward 1963.7694091796875\n",
      "reward 1922.098876953125\n",
      "reward 1934.8465576171875\n",
      "reward 1922.1207275390625\n",
      "reward 1945.76953125\n",
      "reward 1941.7449951171875\n",
      "reward 2046.1741943359375\n",
      "reward 1941.7156982421875\n",
      "reward 2016.767578125\n",
      "reward 2043.349853515625\n",
      "reward 2122.3359375\n",
      "reward 2074.270751953125\n",
      "reward 2124.569580078125\n",
      "reward 2172.501708984375\n",
      "reward 2009.1893310546875\n",
      "reward 2133.870361328125\n",
      "reward 2138.206298828125\n",
      "reward 2072.635498046875\n",
      "reward 2173.37060546875\n",
      "reward 2190.499267578125\n",
      "reward 2215.10595703125\n",
      "reward 2204.335205078125\n",
      "reward 2182.895263671875\n",
      "reward 2219.466064453125\n",
      "reward 2218.315185546875\n",
      "reward 2265.05712890625\n",
      "reward 2291.012451171875\n",
      "reward 2237.629150390625\n",
      "reward 2167.53173828125\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:um6t01sj) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.195 MB of 0.195 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b8886c70c06143238183d3fba6998540"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B sync reduced upload amount by 2.3%             "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_log_std</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/episode_reward</td><td>▁▁▂▂▂▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇██████</td></tr><tr><td>lyapunov_factor</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_log_std</td><td>-3.08809</td></tr><tr><td>eval/episode_reward</td><td>2167.53174</td></tr><tr><td>lyapunov_factor</td><td>0.88</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">feb21-crt*:lr=(0.003, 0.001),tr=100,lf=0.88,td=0.98</strong> at: <a href='https://wandb.ai/knyaz/grads/runs/um6t01sj' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/um6t01sj</a><br/> View job at <a href='https://wandb.ai/knyaz/grads/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MTYxNTU1MA==/version_details/v19' target=\"_blank\">https://wandb.ai/knyaz/grads/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MTYxNTU1MA==/version_details/v19</a><br/>Synced 6 W&B file(s), 0 media file(s), 5 artifact file(s) and 1 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240222_232948-um6t01sj/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:um6t01sj). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112397511412079, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "38a97472be1b4ac4b1491071dfdb61b4"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/home/knyaz/workspace/grads/wandb/run-20240223_030641-owivfuf5</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/knyaz/grads/runs/owivfuf5' target=\"_blank\">feb21-crt*:lr=(0.003, 0.001),tr=100,lf=0.9,td=0.98</a></strong> to <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">https://wandb.ai/knyaz/grads</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/knyaz/grads/runs/owivfuf5' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/owivfuf5</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished tracing + transforming training_epoch for pjit in 8.498022079467773 sec\n",
      "Compiling training_epoch for with global shapes and types [ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[]), ShapedArray(uint32[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "Finished jaxpr to MLIR module conversion jit(training_epoch) in 6.700535297393799 sec\n",
      "Finished XLA compilation of jit(training_epoch) in 52.45881962776184 sec\n",
      "Finished tracing + transforming evaluate for pjit in 1.1037039756774902 sec\n",
      "Compiling evaluate for with global shapes and types [ShapedArray(uint32[2]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "Finished jaxpr to MLIR module conversion jit(evaluate) in 0.9817953109741211 sec\n",
      "Finished XLA compilation of jit(evaluate) in 4.961519479751587 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 124.2914047241211\n",
      "reward 144.7252655029297\n",
      "reward 214.2501220703125\n",
      "reward 234.96641540527344\n",
      "reward 269.0244445800781\n",
      "reward 310.4577941894531\n",
      "reward 344.1134033203125\n",
      "reward 360.2431335449219\n",
      "reward 406.2337341308594\n",
      "reward 429.2743835449219\n",
      "reward 542.6453247070312\n",
      "reward 553.2809448242188\n",
      "reward 605.5625\n",
      "reward 570.3961791992188\n",
      "reward 561.9203491210938\n",
      "reward 720.5319213867188\n",
      "reward 725.14208984375\n",
      "reward 783.0523071289062\n",
      "reward 866.9998168945312\n",
      "reward 836.78125\n",
      "reward 851.4298095703125\n",
      "reward 903.7099609375\n",
      "reward 975.3214721679688\n",
      "reward 982.5113525390625\n",
      "reward 1013.8851928710938\n",
      "reward 1089.4703369140625\n",
      "reward 1106.2403564453125\n",
      "reward 1124.2288818359375\n",
      "reward 1145.197265625\n",
      "reward 1090.099853515625\n",
      "reward 1184.1888427734375\n",
      "reward 1263.5054931640625\n",
      "reward 1225.732666015625\n",
      "reward 1329.6669921875\n",
      "reward 1309.9637451171875\n",
      "reward 1283.4759521484375\n",
      "reward 1340.611572265625\n",
      "reward 1440.919921875\n",
      "reward 1401.9215087890625\n",
      "reward 1463.9228515625\n",
      "reward 1509.4005126953125\n",
      "reward 1466.7353515625\n",
      "reward 1451.6707763671875\n",
      "reward 1362.696044921875\n",
      "reward 1540.7708740234375\n",
      "reward 1498.5904541015625\n",
      "reward 1555.9520263671875\n",
      "reward 1441.9920654296875\n",
      "reward 1525.4085693359375\n",
      "reward 1598.111572265625\n",
      "reward 1560.4412841796875\n",
      "reward 1592.869873046875\n",
      "reward 1532.636474609375\n",
      "reward 1624.7630615234375\n",
      "reward 1593.900146484375\n",
      "reward 1489.007080078125\n",
      "reward 1650.7230224609375\n",
      "reward 1575.625244140625\n",
      "reward 1633.006591796875\n",
      "reward 1544.7459716796875\n",
      "reward 1542.947265625\n",
      "reward 1683.3050537109375\n",
      "reward 1724.0491943359375\n",
      "reward 1645.5472412109375\n",
      "reward 1664.5386962890625\n",
      "reward 1681.4429931640625\n",
      "reward 1869.846923828125\n",
      "reward 1762.041259765625\n",
      "reward 1783.780517578125\n",
      "reward 1732.490478515625\n",
      "reward 1723.5672607421875\n",
      "reward 1751.009033203125\n",
      "reward 1788.702392578125\n",
      "reward 1621.5703125\n",
      "reward 1807.257080078125\n",
      "reward 1743.1383056640625\n",
      "reward 1713.7723388671875\n",
      "reward 1634.0035400390625\n",
      "reward 1857.6873779296875\n",
      "reward 1911.9879150390625\n",
      "reward 1892.9906005859375\n",
      "reward 1811.4305419921875\n",
      "reward 2043.104736328125\n",
      "reward 1837.0064697265625\n",
      "reward 1713.962890625\n",
      "reward 2007.7769775390625\n",
      "reward 1762.6800537109375\n",
      "reward 1894.6893310546875\n",
      "reward 1885.956298828125\n",
      "reward 1900.1785888671875\n",
      "reward 1867.8680419921875\n",
      "reward 1892.4898681640625\n",
      "reward 1898.6058349609375\n",
      "reward 1916.2913818359375\n",
      "reward 1958.6636962890625\n",
      "reward 1802.634033203125\n",
      "reward 1764.434814453125\n",
      "reward 1787.4139404296875\n",
      "reward 1890.93359375\n",
      "reward 1979.3861083984375\n",
      "reward 1854.7474365234375\n",
      "reward 1814.971923828125\n",
      "reward 2002.0836181640625\n",
      "reward 1920.9276123046875\n",
      "reward 1988.4453125\n",
      "reward 2120.955322265625\n",
      "reward 2157.230224609375\n",
      "reward 2142.268310546875\n",
      "reward 2073.110107421875\n",
      "reward 2249.818359375\n",
      "reward 2280.0166015625\n",
      "reward 2103.911376953125\n",
      "reward 2244.584716796875\n",
      "reward 2320.69677734375\n",
      "reward 2279.259765625\n",
      "reward 2256.696044921875\n",
      "reward 2208.337890625\n",
      "reward 2346.7080078125\n",
      "reward 2051.97900390625\n",
      "reward 2194.58642578125\n"
     ]
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Finishing last run (ID:owivfuf5) before initializing another..."
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='0.121 MB of 0.121 MB uploaded\\r'), FloatProgress(value=1.0, max=1.0)))",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6c295f87ba4841708daf27d9d103c838"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "W&B sync reduced upload amount by 3.2%             "
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>agent_log_std</td><td>██▇▇▇▇▆▆▆▆▅▅▅▅▅▄▄▄▄▄▃▃▃▃▃▃▂▂▂▂▂▂▂▂▁▁▁▁▁▁</td></tr><tr><td>eval/episode_reward</td><td>▁▁▂▂▃▃▃▄▄▄▄▅▅▅▅▆▆▆▆▆▆▆▆▆▆▆▇▇▇▇▇▇▆▇▇█████</td></tr><tr><td>lyapunov_factor</td><td>▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>agent_log_std</td><td>-3.08806</td></tr><tr><td>eval/episode_reward</td><td>2194.58643</td></tr><tr><td>lyapunov_factor</td><td>0.9</td></tr></table><br/></div></div>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run <strong style=\"color:#cdcd00\">feb21-crt*:lr=(0.003, 0.001),tr=100,lf=0.9,td=0.98</strong> at: <a href='https://wandb.ai/knyaz/grads/runs/owivfuf5' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/owivfuf5</a><br/> View job at <a href='https://wandb.ai/knyaz/grads/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MTYxNTU1MA==/version_details/v20' target=\"_blank\">https://wandb.ai/knyaz/grads/jobs/QXJ0aWZhY3RDb2xsZWN0aW9uOjE0MTYxNTU1MA==/version_details/v20</a><br/>Synced 6 W&B file(s), 0 media file(s), 3 artifact file(s) and 0 other file(s)"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Find logs at: <code>./wandb/run-20240223_030641-owivfuf5/logs</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Successfully finished last run (ID:owivfuf5). Initializing new run:<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011112205944179247, max=1.0…",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d78673bcb5df454a9bd62245c4a68334"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "wandb version 0.16.3 is available!  To upgrade, please run:\n $ pip install wandb --upgrade"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Tracking run with wandb version 0.16.1"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Run data is saved locally in <code>/var/home/knyaz/workspace/grads/wandb/run-20240223_064120-a8ja4uvk</code>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": "Syncing run <strong><a href='https://wandb.ai/knyaz/grads/runs/a8ja4uvk' target=\"_blank\">feb21-crt*:lr=(0.003, 0.001),tr=100,lf=0.92,td=0.98</a></strong> to <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View project at <a href='https://wandb.ai/knyaz/grads' target=\"_blank\">https://wandb.ai/knyaz/grads</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "<IPython.core.display.HTML object>",
      "text/html": " View run at <a href='https://wandb.ai/knyaz/grads/runs/a8ja4uvk' target=\"_blank\">https://wandb.ai/knyaz/grads/runs/a8ja4uvk</a>"
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finished tracing + transforming training_epoch for pjit in 9.435025691986084 sec\n",
      "Compiling training_epoch for with global shapes and types [ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[]), ShapedArray(int32[]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[]), ShapedArray(uint32[2])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "Finished jaxpr to MLIR module conversion jit(training_epoch) in 7.152482032775879 sec\n",
      "Finished XLA compilation of jit(training_epoch) in 56.174681663513184 sec\n",
      "Finished tracing + transforming evaluate for pjit in 1.0942630767822266 sec\n",
      "Compiling evaluate for with global shapes and types [ShapedArray(uint32[2]), ShapedArray(float32[64,27]), ShapedArray(float32[64]), ShapedArray(float32[64,64]), ShapedArray(float32[64]), ShapedArray(float32[8,64]), ShapedArray(float32[8]), ShapedArray(float32[8]), ShapedArray(float32[27]), ShapedArray(float32[27]), ShapedArray(int32[])]. Argument mapping: (GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated}), GSPMDSharding({replicated})).\n",
      "Finished jaxpr to MLIR module conversion jit(evaluate) in 0.7479338645935059 sec\n",
      "Finished XLA compilation of jit(evaluate) in 5.020232677459717 sec\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reward 129.54931640625\n",
      "reward 176.2671356201172\n"
     ]
    }
   ],
   "source": [
    "for lr in [(3e-3, 1e-3)]:\n",
    "    for lf in [0.88, 0.9, 0.92]:\n",
    "        env = envs.create(env_name=\"ant\", backend=\"spring\")\n",
    "        env = envs.training.wrap(env, episode_length=1000)\n",
    "        env = envs.training.EvalWrapper(env)\n",
    "\n",
    "        learning_rate = lr\n",
    "        truncation_length = 100\n",
    "        num_envs = 40\n",
    "        episode_length = 400\n",
    "        max_gradient_norm = 1.0\n",
    "        lyapunov_factor = lf\n",
    "        wd = 1e-5\n",
    "        time_discount = 0.98\n",
    "        num_epochs = 1200\n",
    "\n",
    "        wandb.init(\n",
    "            # Set the project where this run will be logged\n",
    "            project=\"grads\",\n",
    "            save_code=True,\n",
    "            name=f\"feb21-crt*:lr={learning_rate},tr={truncation_length},lf={lyapunov_factor},td={time_discount}\",\n",
    "            config = {\n",
    "                \"learning_rate\": learning_rate,\n",
    "                \"truncation_length\": truncation_length,\n",
    "                \"num_envs\": num_envs,\n",
    "                \"episode_length\": episode_length,\n",
    "                \"max_gradient_norm\": max_gradient_norm,\n",
    "                \"lyapunov_factor\": lyapunov_factor,\n",
    "                \"weight_decay\": wd\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        \n",
    "        def run(env=env):\n",
    "            agent = Actor(key=jr.PRNGKey(42), observation_size=env.observation_size, action_size=env.action_size)\n",
    "            \n",
    "            evaluator = Evaluator(\n",
    "                env,\n",
    "                agent,\n",
    "                num_eval_envs=num_envs,\n",
    "                episode_length=episode_length,\n",
    "            )\n",
    "            \n",
    "            if isinstance(learning_rate, tuple):\n",
    "                lr_start, lr_end = learning_rate\n",
    "            else:\n",
    "                lr_start = learning_rate\n",
    "                lr_end = learning_rate\n",
    "            schedule = optax.cosine_decay_schedule(init_value=lr_start, alpha=lr_end / lr_start, decay_steps=num_epochs)\n",
    "\n",
    "            optimizer = optax.chain(\n",
    "                optax.clip(max_gradient_norm),\n",
    "                optax.adamw(learning_rate=schedule, weight_decay=wd)\n",
    "            )\n",
    "            opt_state = optimizer.init(agent.get_trainable())\n",
    "\n",
    "            local_key = jr.PRNGKey(44)\n",
    "\n",
    "            metrics = {}\n",
    "            for it in range(num_epochs):\n",
    "                # optimization\n",
    "                epoch_key, local_key = jax.random.split(local_key)\n",
    "                agent, opt_state, other = training_epoch(agent, opt_state, key=epoch_key, optimizer=optimizer, epoch_index=jnp.array(it, dtype=jnp.int32))\n",
    "\n",
    "                if it % 10 == 3:\n",
    "                    metrics = evaluator.run_evaluation(agent=agent, training_metrics=metrics, key=jr.PRNGKey(42))\n",
    "                    print(f'reward {metrics[\"eval/episode_reward\"]}')\n",
    "                    wandb.log({\"eval/episode_reward\": metrics[\"eval/episode_reward\"], \"agent_log_std\": agent.log_std.mean(), \"lyapunov_factor\": lyapunov_schedule(lyapunov_factor, it)})\n",
    "                \n",
    "            return agent, metrics\n",
    "                \n",
    "        out = run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96534349d24129",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175522225Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "387fa01cdf255d3e",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175637605Z"
    }
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "truncation_length = 15\n",
    "num_envs = 50\n",
    "episode_length = 50\n",
    "max_gradient_norm = 1.0\n",
    "\n",
    "agent = out[0]\n",
    "\n",
    "eval_env = training.wrap(\n",
    "    env,\n",
    "    episode_length=episode_length,\n",
    ")\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    env,\n",
    "    agent,\n",
    "    num_eval_envs=num_envs,\n",
    "    episode_length=episode_length,\n",
    ")\n",
    "\n",
    "metrics = evaluator.run_evaluation(agent=agent, training_metrics={}, key=jr.PRNGKey(42))\n",
    "print(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d7c318e652cf313",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175712470Z"
    }
   },
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3da42deec8b78",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175742876Z"
    }
   },
   "outputs": [],
   "source": [
    "out = grad_and_value(\n",
    "    key=jr.PRNGKey(48),\n",
    "    env=env,\n",
    "    env_state=state,\n",
    "    agent=agent,\n",
    "    unroll_length=15,\n",
    ")\n",
    "# check that the grads are not zeros\n",
    "assert jnp.any(out[1].mean_network.structure[0].weight != 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e97606d895ffc1b9",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175776840Z"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm as tqdm\n",
    "import optax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3cc80cc83c7fe3",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175809379Z"
    }
   },
   "outputs": [],
   "source": [
    "key = jr.PRNGKey(42)\n",
    "opt = optax.adam(1e-3)\n",
    "opt_state = opt.init(eqx.filter(agent.get_trainable(), eqx.is_inexact_array))\n",
    "for i in range(100):\n",
    "    #key, subkey = jr.split(key)\n",
    "    (rewards, trs), out = get_grad(\n",
    "        key=key, \n",
    "        agent=agent, \n",
    "        env_state=env.reset(jr.split(key, 256)),\n",
    "        env=env, \n",
    "        unroll_length=10,\n",
    "        N=256,\n",
    "        factor=jnp.array(1.0)\n",
    "    )\n",
    "    fi_grad = jtu.tree_map(lambda x: -jnp.mean(x, axis=0), out, is_leaf=eqx.is_array)\n",
    "    updates, opt_state = opt.update(fi_grad, opt_state)\n",
    "    print(rewards.mean())\n",
    "\n",
    "    agent = eqx.apply_updates(agent, updates)\n",
    "    new_normalizer = agent.normalizer.update_batched(trs.observation.reshape((-1, env.observation_size)))\n",
    "    agent = eqx.tree_at(\n",
    "        where=lambda s: s.normalizer,\n",
    "        pytree=agent,\n",
    "        replace_fn=lambda _: new_normalizer,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8440e87d202a2ad8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175860849Z"
    }
   },
   "outputs": [],
   "source": [
    "from brax.training.agents.apg import train as apg\n",
    "from brax import envs\n",
    "from jax import tree_util as jtu\n",
    "\n",
    "rew = 0\n",
    "def progress(*args):\n",
    "    global rew\n",
    "    rew += args[1]['eval/episode_reward']\n",
    "    if args[0] % 10 == 0:\n",
    "        print(f'reward: {rew / 10}')\n",
    "        rew = 0\n",
    "\n",
    "train_fn = jtu.Partial( apg.train, episode_length=400, action_repeat=1, num_envs=40, num_eval_envs = 10, learning_rate=3e-3, truncation_length = 12, normalize_observations = True, progress_fn = progress, num_evals=1000)\n",
    "\n",
    "train_fn(environment=envs.create(env_name=\"ant\", backend=\"spring\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "101942e3c6148fa8",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175889817Z"
    }
   },
   "outputs": [],
   "source": [
    "st = env.reset(jr.split(jr.PRNGKey(42), 128))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bdc1f6bf351aabb",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175912588Z"
    }
   },
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"\n",
    "    Evaluates agent on the environment.\n",
    "    It is not jittable, since run_evaluation needs time.time()\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, eval_env, num_eval_envs, episode_length):\n",
    "        self._eval_walltime = 0.0\n",
    "        self.eval_env = envs.training.EvalWrapper(eval_env)\n",
    "        self.episode_length = episode_length\n",
    "        self.num_eval_envs = num_eval_envs\n",
    "        self._steps_per_unroll = episode_length * num_eval_envs\n",
    "\n",
    "    @eqx.filter_jit\n",
    "    def evaluate(self, key: jr.PRNGKey, agent):\n",
    "        reset_keys = jr.split(key, self.num_eval_envs)\n",
    "        eval_first_state = self.eval_env.reset(reset_keys)\n",
    "        return policy_unroll(\n",
    "            key=key,\n",
    "            env=self.eval_env,\n",
    "            env_state=eval_first_state,\n",
    "            agent=agent,\n",
    "            unroll_length=self.episode_length,\n",
    "            extra_fields=(\"truncation\",),\n",
    "            perturbation=get_perturbation_from_agent(agent),\n",
    "        )[0]\n",
    "\n",
    "    def run_evaluation(\n",
    "            self, key: jr.PRNGKey, agent, training_metrics, aggregate_episodes: bool = True\n",
    "    ):\n",
    "        t = time.time()\n",
    "        eval_state = self.evaluate(key, agent)\n",
    "        print(eval_state)\n",
    "        eval_metrics = eval_state.info[\"eval_metrics\"]\n",
    "        eval_metrics.active_episodes.block_until_ready()\n",
    "        epoch_eval_time = time.time() - t\n",
    "        metrics = {}\n",
    "        for fn in [jnp.mean, jnp.std]:\n",
    "            suffix = \"_std\" if fn == jnp.std else \"\"\n",
    "            metrics.update(\n",
    "                {\n",
    "                    f\"eval/episode_{name}{suffix}\": (\n",
    "                        fn(value) if aggregate_episodes else value\n",
    "                    )\n",
    "                    for name, value in eval_metrics.episode_metrics.items()\n",
    "                }\n",
    "            )\n",
    "        metrics[\"eval/avg_episode_length\"] = jnp.mean(eval_metrics.episode_steps)\n",
    "        metrics[\"eval/epoch_eval_time\"] = epoch_eval_time\n",
    "        metrics[\"eval/sps\"] = self._steps_per_unroll / epoch_eval_time\n",
    "        self._eval_walltime = self._eval_walltime + epoch_eval_time\n",
    "        metrics = {\"eval/walltime\": self._eval_walltime, **training_metrics, **metrics}\n",
    "\n",
    "        return metrics\n",
    "\n",
    "eval_env = envs.create(env_name=\"ant\", backend=\"spring\")\n",
    "eval_env = envs.training.wrap(eval_env, episode_length=1000)\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    eval_env=eval_env,\n",
    "    num_eval_envs=16,\n",
    "    episode_length=1000,\n",
    ")\n",
    "\n",
    "evaluator.run_evaluation(key=jr.PRNGKey(42), agent=agent, training_metrics={})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59e763e692af1ac",
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-22T22:29:32.175935026Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
